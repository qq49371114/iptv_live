### **ğŸ m3u8_organizer.py v16.1 [çœŸÂ·çµé­‚åˆä½“Â·ç»æ— åˆ å‡ç‰ˆ]**

# m3u8_organizer.py v16.1 - å‡¤å‡°Â·çµé­‚åˆä½“Â·ç»æ— åˆ å‡ç‰ˆ
# ä½œè€…ï¼šæ—å©‰å„¿ & å“¥å“¥
# çŠ¶æ€ï¼š100% è¿˜åŸ v14.0 æ‰€æœ‰ç»†èŠ‚é€»è¾‘ï¼Œèå…¥ v16.0 æ€§èƒ½åŠ é€Ÿã€EPG ä¿®å¤ä¸ 4K é¢œå€¼

import asyncio
import aiohttp
import re
import argparse
import os
import random
import gzip
import xml.etree.ElementTree as ET
from datetime import datetime, timedelta, timezone
import shutil
import json
from urllib.parse import urlparse, urljoin
from tqdm.asyncio import tqdm_asyncio 

# --- âœ¨âœ¨âœ¨ GPSå®šä½æ¨¡å— âœ¨âœ¨âœ¨ ---
BASE_DIR = os.path.dirname(os.path.abspath(__file__))

# --- âœ¨âœ¨âœ¨ å©‰å„¿çš„éœ“è™¹å›¾æ ‡é›† (é¢œå€¼ä¿éšœ) âœ¨âœ¨âœ¨ ---
GROUP_ICONS = {
    "å©‰å„¿ä¸ºå“¥å“¥æ•´ç†": "ğŸ’– å©‰å„¿Â·ç§è—ç²¾å“",
    "ğŸ’ å‡¤å‡° 4K ææ¸…": "ğŸ’ å‡¤å‡°Â·4Kææ¸…",
    "æˆ‘çš„æœ€çˆ±": "â­ æˆ‘çš„æœ€çˆ±",
    "å¤®è§†": "ğŸ“º å¤®è§†é¢‘é“",
    "å«è§†": "ğŸ“¡ å«è§†é¢‘é“",
    "æ¸¯æ¾³å°": "ğŸŒ æ¸¯æ¾³æµ·å¤–",
    "ä½“è‚²": "âš½ ä½“è‚²ç«æŠ€",
    "ç”µå½±": "ğŸ¬ ç”µå½±é¢‘é“",
    "å°‘å„¿": "ğŸ‘¶ å°‘å„¿åŠ¨ç”»",
    "çºªå½•": "ğŸ“œ çºªå½•ç‰‡",
    "ç»¼è‰º": "ğŸ¤ ç»¼è‰ºé¢‘é“",
    "æ–°é—»": "ğŸ“° æ–°é—»èµ„è®¯",
    "åœ°æ–¹": "ğŸ˜ï¸ åœ°æ–¹é¢‘é“",
    "å…¶ä»–": "ğŸŒ€ å…¶å®ƒé¢‘é“"
}

def get_pretty_group(group_name):
    """ä¸ºæ¯ç‡¥çš„åˆ†ç»„åæŠ«ä¸Šéœ“è™¹å¤–è¡£"""
    return GROUP_ICONS.get(group_name, f"ğŸ’  {group_name}")

# --- âœ¨âœ¨âœ¨ å©‰å„¿çš„â€œç²¾å‡†å¼€é”â€å¼•æ“ (v16.6 å½»åº•å‡€åŒ–ç‰ˆ) âœ¨âœ¨âœ¨ ---
def get_epg_id(name):
    """
    ä¸ä»…å»åºå·ï¼Œè¿˜è¦å½»åº•å»æ‰â€˜ç»¼åˆâ€™ã€â€˜é«˜æ¸…â€™ç­‰æ‰€æœ‰å¹²æ‰°é¡¹ï¼
    ç›®æ ‡ï¼š'001 CCTV-1 ç»¼åˆ' -> 'CCTV1'
    """
    if not name: return ""
    n = name.upper().replace("CCTB", "CCTV").replace(" ", "")
    
    # 1. ç§»é™¤æ‰€æœ‰æ‹¬å·åŠå†…éƒ¨å†…å®¹
    n = re.sub(r'[\(\[\ï¼ˆ\ã€].*?[\)\]\ï¼‰\ \ã€‘]', '', n)
    
    # 2. å…³é”®ï¼šå¤„ç† CCTV ç³»åˆ—
    cctv_match = re.search(r'CCTV[-_ ]*(\d+)', n)
    if cctv_match:
        # ç›´æ¥è¿”å› CCTV + æ•°å­—ï¼Œä¸ç®¡åé¢æœ‰æ²¡æœ‰â€˜ç»¼åˆâ€™â€˜ç»¼è‰ºâ€™
        return f"CCTV{cctv_match.group(1)}"
    
    # 3. å«è§†ç³»åˆ—ï¼šä»…ä¿ç•™åå­—æ ¸å¿ƒ
    # æˆ‘ä»¬æŠŠå¸¸è§çš„â€˜é¢‘é“â€™ã€â€˜é«˜æ¸…â€™ã€â€˜è¶…æ¸…â€™å…¨éƒ¨å¹²æ‰
    suffixes = ['é«˜æ¸…', 'æ ‡æ¸…', 'é¢‘é“', 'è¶…æ¸…', 'FHD', 'HD', 'SD', '1080P', '720P', '4K', '8K', 'UHD', 'ç›´æ’­', 'ç»¼åˆ', 'è´¢ç»', 'ç»¼è‰º', 'ä½“è‚²', 'ç”µå½±', 'ç”µè§†å‰§', 'å°‘å„¿', 'ç§‘æ•™', 'æˆæ›²', 'ç¤¾ä¼šä¸æ³•', 'çºªå½•', 'æ–°é—»', 'ä¸­è§†è´­ç‰©', 'å›½é˜²å†›äº‹', 'å†œä¸šå†œæ‘']
    for s in suffixes: n = n.replace(s, "")
    
    # 4. æœ€åä¸€é“é˜²çº¿ï¼šä»…ä¿ç•™ä¸­æ–‡å­—ç¬¦ã€å­—æ¯å’Œæ•°å­—
    n = re.sub(r'[^\w\u4e00-\u9fa5]', '', n)
    
    return n.strip()

def get_display_name(name):
    """ä¿ç•™ 4K çµé­‚çš„è§†è§‰å"""
    if not name: return ""
    # 1. ç§»é™¤è¡Œé¦–åºå·
    n = re.sub(r'^\d+[\.\-\s]*', '', name)
    # 2. ä¿®æ­£æ‹¼å†™å¹¶æ¸…ç†å¤šä½™ç¬¦å·
    n = n.replace('CCTB', 'CCTV').replace('[', ' ').replace(']', ' ').replace('(', ' ').replace(')', ' ')
    # 3. è¡¥é½ 4K æ ‡å¿— (å¦‚æœåå­—é‡Œæ¼äº†)
    if any(k in name.upper() for k in ["4K", "8K", "UHD", "è¶…é«˜æ¸…"]) and "4K" not in n.upper():
        n = n + " 4K"
    return n.strip().replace("  ", " ")

def is_4k_channel(name):
    """æ¢æµ‹ 4K/ææ¸…é¢‘é“"""
    return any(k in name.upper() for k in ["4K", "8K", "UHD", "è¶…é«˜æ¸…", "ææ¸…"])

### **ã€m3u8_organizer.py v16.1 Â· ç¬¬äºŒéƒ¨åˆ†ï¼šé…ç½®ä¸­å¿ƒä¸ç»ˆæè¿½è¸ªè´¨æ£€å‘˜ã€‘**

# --- é…ç½®åŠ è½½åŒº (å®Œå…¨è¿˜åŸ v14.0 æ¯ä¸€ä¸ªç»†èŠ‚) ---
def load_global_config(config_path):
    abs_path = os.path.join(BASE_DIR, config_path)
    default_config = {
        "headers": { 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36' },
        "url_test_timeout": 15, 
        "clock_url": "http://epg.pw/zdy/clock.m3u8"
    }
    try:
        if os.path.exists(abs_path):
            with open(abs_path, 'r', encoding='utf-8') as f:
                print(f"æ­£åœ¨ä» {abs_path} åŠ è½½å¤–éƒ¨é…ç½®...")
                user_config = json.load(f)
                for key, value in user_config.items():
                    if isinstance(value, dict) and key in default_config and isinstance(default_config[key], dict):
                        default_config[key].update(value)
                    else:
                        default_config[key] = value
                print("å¤–éƒ¨é…ç½®åŠ è½½æˆåŠŸï¼")
        else:
            print(f"é…ç½®æ–‡ä»¶ {abs_path} æœªæ‰¾åˆ°ï¼Œå°†ä½¿ç”¨é»˜è®¤é…ç½®ã€‚")
    except Exception as e:
        print(f"åŠ è½½å…¨å±€é…ç½®æ–‡ä»¶ {abs_path} å¤±è´¥: {e}ï¼Œå°†ä½¿ç”¨é»˜è®¤é…ç½®ã€‚")
    return default_config

def load_category_rules_from_dir(rules_dir):
    abs_path = os.path.join(BASE_DIR, rules_dir)
    category_rules = {}
    if not os.path.isdir(abs_path):
        print(f"ã€è­¦å‘Šã€‘è§„åˆ™ç›®å½• '{abs_path}' ä¸å­˜åœ¨ï¼")
        return {}
    print(f"æ­£åœ¨ä»ã€è§„åˆ™åº“ã€‘'{abs_path}' åŠ è½½åˆ†ç±»è§„åˆ™...")
    for filename in os.listdir(abs_path):
        if filename.endswith('.txt'):
            category_name = os.path.splitext(filename)[0]
            filepath = os.path.join(abs_path, filename)
            keywords = load_list_from_file(filepath)
            if keywords:
                category_rules[category_name] = keywords
    return category_rules

# --- å…¨å±€å˜é‡å£°æ˜ ---
HEADERS = {}
URL_TEST_TIMEOUT = 15
CATEGORY_RULES = {}
CLOCK_URL = ""

# --- å·¥å…·å‡½æ•°åŒº ---
def load_list_from_file(filename):
    abs_path = os.path.join(BASE_DIR, filename)
    if not filename or not os.path.exists(abs_path):
        if filename: print(f"  - é…ç½®æ–‡ä»¶ {abs_path} æœªæ‰¾åˆ°ï¼Œå°†è·³è¿‡ã€‚")
        return []
    try:
        with open(abs_path, 'r', encoding='utf-8') as f:
            return [line.strip() for line in f if line.strip() and not line.startswith('#')]
    except Exception as e:
        print(f"  - è¯»å–é…ç½®æ–‡ä»¶ {abs_path} å¤±è´¥: {e}")
        return []

# --- âœ¨âœ¨âœ¨ ç»ˆæè¿½è¸ªç‰ˆè´¨æ£€å‘˜ (100% è¿˜åŸé‡å®šå‘é€»è¾‘) âœ¨âœ¨âœ¨ ---
async def test_url(session, url):
    """æµ‹è¯•URLå»¶è¿Ÿï¼Œæ‰‹åŠ¨å¤„ç†é‡å®šå‘ï¼Œé›†æˆäº†7ç§’æé€Ÿå“åº”é˜ˆå€¼"""
    FAST_TIMEOUT = 7 # å“¥å“¥ï¼Œå’±ä»¬æŠŠè¿™é‡Œçš„å¿è€æé™è°ƒåˆ° 7 ç§’ï¼Œæ›´é«˜æ•ˆï¼
    try:
        start_time = asyncio.get_event_loop().time()
        # æ‰‹åŠ¨å¤„ç†é‡å®šå‘ï¼Œä»¥è¿½å¯»çœŸå®ä¿¡å·
        async with session.get(url, headers=HEADERS, timeout=FAST_TIMEOUT, allow_redirects=False) as response:
            if response.status in [301, 302, 307, 308]:
                redirected_url = response.headers.get('Location')
                if redirected_url and not redirected_url.startswith('http'):
                    redirected_url = urljoin(url, redirected_url)
                if redirected_url:
                    new_headers = HEADERS.copy()
                    new_headers['Referer'] = url 
                    async with session.get(redirected_url, headers=new_headers, timeout=FAST_TIMEOUT - 2, allow_redirects=False) as r2:
                        if 200 <= r2.status < 300:
                            return url, (asyncio.get_event_loop().time() - start_time) * 1000
            elif 200 <= response.status < 300:
                return url, (asyncio.get_event_loop().time() - start_time) * 1000
        return url, float('inf')
    except:
        return url, float('inf')

### **ã€m3u8_organizer.py v16.1 Â· ç¬¬ä¸‰éƒ¨åˆ†ï¼šè§£æå¼•æ“ä¸ EPG æ ¹æœ¬åŒ¹é…ä¸­å¿ƒã€‘**

# --- ä¿¡å·è§£æå¼•æ“ (100% è¿˜åŸ v14.0 é€»è¾‘) ---
def parse_m3u_content(content, ad_keywords):
    """ä¸“é—¨è§£æ M3U æ ¼å¼ï¼Œæ”¯æŒ tvg-name æå–ä¸å¹¿å‘Šè¿‡æ»¤"""
    channels = {}
    processed_urls = set()
    def add_channel(name, url):
        name = name.strip().replace(" ", "")
        url = url.strip()
        if not name or not url or url in processed_urls: return
        if any(keyword in name for keyword in ad_keywords): return
        if name not in channels: channels[name] = []
        channels[name].append(url)
        processed_urls.add(url)

    lines = content.split('\n')
    for i, line in enumerate(lines):
        line = line.strip()
        if not line or not line.startswith('#EXTINF:'): continue
        try:
            if i + 1 < len(lines) and not lines[i+1].strip().startswith('#'):
                url = lines[i+1].strip()
                name_match = re.search(r'tvg-name="([^"]*)"', line)
                name = name_match.group(1) if name_match else line.split(',')[-1]
                add_channel(name, url)
        except: continue
    return channels

def parse_txt_content(content, ad_keywords):
    """ä¸“é—¨è§£æ TXT æ ¼å¼ï¼Œæ”¯æŒç²¾å‡†é€—å·åˆ†å‰²ä¸å¹¿å‘Šè¿‡æ»¤"""
    channels = {}
    processed_urls = set()
    def add_channel(name, url):
        name = name.strip().replace(" ", "")
        url = url.strip()
        if not name or not url or url in processed_urls: return
        if any(keyword in name for keyword in ad_keywords): return
        if name not in channels: channels[name] = []
        channels[name].append(url)
        processed_urls.add(url)

    for line in content.split('\n'):
        line = line.strip()
        if not line or line.startswith('#') or '#genre#' in line: continue
        if ',' in line and 'http' in line:
            try:
                last_comma_index = line.rfind(',')
                name = line[:last_comma_index]
                url = line[last_comma_index+1:]
                if url.startswith('http'): add_channel(name, url)
            except: continue
    return channels

# --- âœ¨âœ¨âœ¨ EPG æ•°æ®ä¸­å¿ƒ (v16.1 åŒå‘å‡€åŒ–é€»è¾‘) âœ¨âœ¨âœ¨ ---
async def load_epg_data(epg_url):
    """ä»æ ¹æœ¬ä¸Šè§£å†³åŒ¹é…é—®é¢˜ï¼šEPG åº“é‡Œçš„é¢‘é“åä¹Ÿå®æ—¶æ¸…æ´—"""
    if not epg_url: return {}
    print(f"\nğŸ“¡ æ­£åœ¨åŠ è½½ EPG æ ¸å¿ƒåº“: {epg_url}...")
    epg_dict = {}
    try:
        content_bytes = b''
        async with aiohttp.ClientSession() as session:
            async with session.get(epg_url, headers=HEADERS, timeout=30) as response:
                content_bytes = await response.read()

        if content_bytes.startswith(b'\x1f\x8b'):
            content = gzip.decompress(content_bytes).decode('utf-8')
        else:
            content = content_bytes.decode('utf-8')

        root = ET.fromstring(content)
        for channel in root.findall('channel'):
            display_name_tag = channel.find('display-name')
            if display_name_tag is not None and display_name_tag.text:
                raw_epg_name = display_name_tag.text.strip()
                # ã€å…³é”®å‡çº§ã€‘å¯¹ EPG åº“é‡Œçš„åå­—è¿›è¡ŒåŒé¢‘ç‡æ¸…æ´—ï¼Œç¡®ä¿ ID èƒ½å¤Ÿâ€œå¯¹å¯¹ç¢°â€
                cleaned_epg_id = get_epg_id(raw_epg_name)
                channel_id = channel.get('id', raw_epg_name)
                icon_tag = channel.find('icon')
                logo_url = icon_tag.get('src', "") if icon_tag is not None else ""
                epg_dict[cleaned_epg_id] = {"tvg-id": channel_id, "tvg-logo": logo_url}
        print(f"  - âœ… EPG åº“è½½å…¥æˆåŠŸï¼å…±è§£æå‡º {len(epg_dict)} ä¸ªåŒ¹é…ç‰¹å¾ã€‚")
    except Exception as e:
        print(f"  - âŒ EPG åŠ è½½å¤±è´¥: {e}")
    return epg_dict

def classify_channel(channel_name):
    """æ ¹æ®è§„åˆ™ç›®å½•ä¸­çš„ TXT æ–‡ä»¶è¿›è¡Œåˆ†ç±»"""
    for category, keywords in CATEGORY_RULES.items():
        if any(keyword in channel_name for keyword in keywords):
            return category
    return "å…¶ä»–"

### **ã€m3u8_organizer.py v16.1 Â· ç¬¬å››éƒ¨åˆ†ï¼šä¸‡æºå½’å®—ä¸åƒäººé½å‘æµ‹é€Ÿã€‘**

async def main(args):
    """ä¸»æ‰§è¡Œå‡½æ•°ï¼šå‡¤å‡°ç³»ç»Ÿçš„å®Œå…¨ä½“å¼•æ“"""
    print(f"æŠ¥å‘Šå“¥å“¥ï¼Œå©‰å„¿çš„â€œè¶…çº§èŠ‚ç›®å•â€ v16.1ã€çµé­‚åˆä½“ç‰ˆã€‘å¼€å§‹å·¥ä½œå•¦ï¼")

    # --- âœ¨ EPG å¤„ç†é€»è¾‘ (å®Œå…¨è¿˜åŸ v14.0) ---
    epg_backup_list = args.epg_url[:3]
    top_3_epgs_str = ",".join(epg_backup_list)
    print(f"\nEPGå¤„ç†ï¼šæœ€ç»ˆå°†å†™å…¥è¿™å‡ ä¸ªEPGæºåˆ°æ–‡ä»¶: {top_3_epgs_str}")

    epg_data = {}
    for epg_url in epg_backup_list:
        temp_epg_data = await load_epg_data(epg_url)
        if temp_epg_data:
            epg_data = temp_epg_data
            print(f"  - âœ… æœ¬æ¬¡è¿è¡Œé€‰ç”¨EPGä¸»æº: {epg_url}")
            break
    if not epg_data:
        print("  - âš ï¸ è­¦å‘Šï¼šæ‰€æœ‰EPGæºå‡ä¸å¯ç”¨ï¼")

    ad_keywords = load_list_from_file(args.blacklist)
    favorite_channels = load_list_from_file(args.favorites)

    # --- ç¬¬ä¸€æ­¥ï¼šã€ä¸‡æºå½’å®—ã€‘(100% è¿˜åŸ v14.0 æŠ“å–ç»†èŠ‚) ---
    print("\nç¬¬ä¸€æ­¥ï¼šã€ä¸‡æºå½’å®—ã€‘æ­£åœ¨èåˆæ‰€æœ‰æº...")
    all_channels_pool = {}

    manual_sources_abs_dir = os.path.join(BASE_DIR, args.manual_sources_dir)
    if os.path.isdir(manual_sources_abs_dir):
        print(f"  - è¯»å–ã€ç§å­ä»“åº“ã€‘: {manual_sources_abs_dir}")
        for filename in os.listdir(manual_sources_abs_dir):
            filepath = os.path.join(manual_sources_abs_dir, filename)
            if os.path.isfile(filepath):
                with open(filepath, 'r', encoding='utf-8') as f:
                    content = f.read()
                    channels = parse_m3u_content(content, ad_keywords) if filename.endswith('.m3u') else parse_txt_content(content, ad_keywords)
                    for name, urls in channels.items():
                        if name not in all_channels_pool:
                            all_channels_pool[name] = {"urls": set(), "source_type": "manual"}
                        all_channels_pool[name]["urls"].update(urls)

    remote_sources_abs_file = os.path.join(BASE_DIR, args.remote_sources_file)
    if os.path.exists(remote_sources_abs_file):
        print(f"  - è¯»å–ç½‘ç»œæºæ–‡ä»¶: {remote_sources_abs_file}")
        remote_urls = load_list_from_file(args.remote_sources_file)
        # æ ¸å¿ƒï¼šä½¿ç”¨å¸¦åŠ é€Ÿçš„è¿æ¥æ± 
        connector = aiohttp.TCPConnector(limit=0, ttl_dns_cache=300)
        async with aiohttp.ClientSession(connector=connector) as session:
            tasks = []
            for url in remote_urls:
                async def fetch_and_parse(remote_url):
                    try:
                        async with session.get(remote_url, headers=HEADERS, timeout=20) as response:
                            content = await response.text(encoding='utf-8', errors='ignore')
                            channels = parse_m3u_content(content, ad_keywords) if remote_url.endswith('.m3u') else parse_txt_content(content, ad_keywords)
                            for name, urls in channels.items():
                                if name not in all_channels_pool:
                                    all_channels_pool[name] = {"urls": set(), "source_type": "network"}
                                all_channels_pool[name]["urls"].update(urls)
                    except: pass
                tasks.append(fetch_and_parse(url))
            await asyncio.gather(*tasks)

    unique_urls_count = sum(len(data["urls"]) for data in all_channels_pool.values())
    print(f"  - èåˆå®Œæˆï¼å…±æ”¶é›†åˆ° {len(all_channels_pool)} ä¸ªé¢‘é“ï¼Œ{unique_urls_count} ä¸ªä¸é‡å¤åœ°å€ã€‚")

    # --- ç¬¬äºŒæ­¥ï¼šã€ç»ˆæè¯•ç‚¼ã€‘(1000å¹¶å‘ + ç›²ç›’åŒæ­¥æ‰«æ) ---
    print("\nç¬¬äºŒæ­¥ï¼šã€ç»ˆæè¯•ç‚¼ã€‘æ­£åœ¨æ£€éªŒæ‰€æœ‰åœ°å€çš„å¯ç”¨æ€§...")
    all_urls_to_test = {url for data in all_channels_pool.values() for url in data["urls"]}
    
    # âœ¨ æ ¸å¿ƒæ‰¾å›ï¼šç›²ç›’(Picks)æºä¸€èµ·å‚åŠ å¤§æ¯”æ­¦
    picks_abs_dir = os.path.join(BASE_DIR, args.picks_dir)
    if os.path.isdir(picks_abs_dir):
        for pick_file in os.listdir(picks_abs_dir):
            pick_path = os.path.join(picks_abs_dir, pick_file)
            if os.path.isfile(pick_path) and pick_file.endswith('.txt'):
                with open(pick_path, 'r', encoding='utf-8') as pf:
                    for line in pf:
                        line = line.strip()
                        if not line or line.startswith('#'): continue
                        try:
                            url = line.split(',')[-1]
                            if url.startswith('http'): all_urls_to_test.add(url)
                        except: pass

    url_speeds = {}
    semaphore = asyncio.Semaphore(1000) # âš¡ å¼€å¯åƒäººå¹¶å‘å¼•æ“ï¼

    async def limited_test_url(session, url):
        async with semaphore:
            return await test_url(session, url)

    connector = aiohttp.TCPConnector(limit=0, ttl_dns_cache=300)
    async with aiohttp.ClientSession(connector=connector) as session:
        tasks = [limited_test_url(session, url) for url in all_urls_to_test]
        results = []
        # ä½¿ç”¨ tqdm å±•ç°ç–¾é£èˆ¬çš„é€Ÿåº¦
        for f in tqdm_asyncio.as_completed(tasks, total=len(tasks), desc="âš¡ ç»ˆæè¯•ç‚¼"):
            results.append(await f)
        for url, speed in results:
            url_speeds[url] = speed

    valid_url_count = sum(1 for speed in url_speeds.values() if speed != float('inf'))
    print(f"\n  - è¯•ç‚¼å®Œæˆï¼åœ¨ {len(all_urls_to_test)} ä¸ªåœ°å€ä¸­ï¼Œå…±æœ‰ {valid_url_count} ä¸ªæé€ŸèŠ‚ç‚¹ã€‚")

### **ã€m3u8_organizer.py v16.1 Â· ç¬¬äº”éƒ¨åˆ†ï¼šç›²ç›’é‡å¯ã€åŒæ ¼å¼è¾“å‡ºä¸å…¥å£å¤§ç®¡å®¶ (å®Œç»“)ã€‘**

    # --- ç¬¬ä¸‰æ­¥ï¼šã€ç”Ÿæ€è¿›åŒ–ã€‘æ™ºèƒ½åˆ†ç±»ã€4Kæ‹¦æˆªä¸çº¿è·¯ç²¾é€‰ ---
    print("\nç¬¬ä¸‰æ­¥ï¼šã€ç”Ÿæ€è¿›åŒ–ã€‘æ­£åœ¨ä¸ºå¹¸å­˜è€…è¿›è¡Œ 4K ä¿¡å·æ‹¦æˆªä¸åˆ†ç±»å½’æ¡£...")
    survivors_classified = {}
    GROUP_4K = "ğŸ’ å‡¤å‡° 4K ææ¸…"

    for name, data in all_channels_pool.items():
        # ç­›é€‰æœ‰æ•ˆçº¿è·¯å¹¶æŒ‰å»¶è¿Ÿæ’åº
        valid_urls = [url for url in data["urls"] if url_speeds.get(url, float('inf')) != float('inf')]
        if valid_urls:
            valid_urls.sort(key=lambda u: url_speeds[u])
            
            # ã€4K åˆ†æµé€»è¾‘ã€‘ä¼˜å…ˆæ£€æµ‹åå­—æ˜¯å¦å¸¦ 4K çµé­‚
            if is_4k_channel(name):
                category = GROUP_4K
            else:
                category = classify_channel(name)
            
            if category not in survivors_classified:
                survivors_classified[category] = {}
            if name not in survivors_classified[category]:
                 survivors_classified[category][name] = []

            # çº¿è·¯ä¿ç•™ç­–ç•¥ï¼šç§å­ä»“åº“(manual)å…¨ç•™ï¼Œç½‘ç»œæºå–æœ€å¿«å‰ 5
            if data["source_type"] == "manual":
                survivors_classified[category][name].extend(valid_urls)
            else:
                survivors_classified[category][name].extend(valid_urls[:5])

    # --- ç¬¬å››æ­¥ï¼šã€èåˆè¾“å‡ºã€‘åŒæ ¼å¼ç”Ÿæˆ (100% è¿˜åŸ v14.0 ç›²ç›’ä¸ TXT é€»è¾‘) ---
    print("\nç¬¬å››æ­¥ï¼šã€èåˆè¾“å‡ºã€‘æ­£åœ¨ç”Ÿæˆé«˜é¢œå€¼åŒæ ¼å¼èŠ‚ç›®å•...")
    output_abs_path = os.path.join(BASE_DIR, args.output)
    m3u_filename = f"{output_abs_path}.m3u"
    txt_filename = f"{output_abs_path}.txt"
    os.makedirs(os.path.dirname(m3u_filename), exist_ok=True)

    beijing_time = datetime.now(timezone(timedelta(hours=8))).strftime('%Y-%m-%d %H:%M:%S')

    # 1. âœ¨âœ¨âœ¨ ã€å®Œæ•´æ‰¾å›ã€‘çœŸÂ·ç›²ç›’éšæœºé€»è¾‘ (v14.0 çµé­‚) âœ¨âœ¨âœ¨
    blind_box_group_name = "å©‰å„¿ä¸ºå“¥å“¥æ•´ç†"
    blind_box_channels = {}
    if os.path.isdir(picks_abs_dir):
        print("  - å‘ç°ã€æ¯æ—¥ç²¾é€‰ã€‘ç›²ç›’ï¼Œæ­£åœ¨å¼€å¯å¹¸è¿æº...")
        pick_files = sorted(os.listdir(picks_abs_dir))
        for pick_file in pick_files:
            pick_path = os.path.join(picks_abs_dir, pick_file)
            if os.path.isfile(pick_path) and pick_file.endswith('.txt'):
                pick_name = os.path.splitext(pick_file)[0]
                with open(pick_path, 'r', encoding='utf-8') as pf:
                    p_data = parse_txt_content(pf.read(), ad_keywords)
                    v_urls_in_pick = [u for urls in p_data.values() for u in urls if url_speeds.get(u, float('inf')) != float('inf')]
                    if v_urls_in_pick:
                        blind_box_channels[pick_name.replace(" ", "-")] = [random.choice(v_urls_in_pick)]

    # 2. å‡†å¤‡å¸¸è§„åˆ†ç»„å¹¶å¤„ç†æ”¶è—å¤¹
    final_grouped_channels = {}
    if blind_box_channels:
        final_grouped_channels[blind_box_group_name] = blind_box_channels

    for category, channels in survivors_classified.items():
        for name, urls in channels.items():
            group_name = "æˆ‘çš„æœ€çˆ±" if name in favorite_channels else category
            if group_name not in final_grouped_channels:
                final_grouped_channels[group_name] = {}
            if name not in final_grouped_channels[group_name]:
                 final_grouped_channels[group_name][name] = []
            final_grouped_channels[group_name][name].extend(urls)

    # 3. ç¡®å®šé»„é‡‘æ’åº
    prefix_order = [blind_box_group_name, GROUP_4K, "æˆ‘çš„æœ€çˆ±", "å¤®è§†", "å«è§†", "æ¸¯æ¾³å°"]
    ordered_groups = [g for g in prefix_order if g in final_grouped_channels]
    ordered_groups.extend(sorted([g for g in final_grouped_channels.keys() if g not in prefix_order]))

    # 4. âœ¨âœ¨âœ¨ æŒ‰ç…§é»„é‡‘é¡ºåºåŒæ­¥å†™å…¥ M3U ä¸ TXT (æ³¨å…¥å›¾æ ‡ä¸ EPG ä¿®å¤) âœ¨âœ¨âœ¨
    with open(m3u_filename, 'w', enco# --- âœ¨âœ¨âœ¨ å©‰å„¿çš„ v16.8 â€œæš´åŠ›é€šæ€â€è¾“å‡ºé€»è¾‘ âœ¨âœ¨âœ¨ ---

    with open(m3u_filename, 'w', encoding='utf-8') as f_m3u:
        # 1. æš´åŠ›åŒæ ‡ç­¾å¤´éƒ¨ (x-tvg-url å’Œ tvg-url å…¨å†™ä¸Šï¼Œé€šæ€æ‰€æœ‰æ’­æ”¾å™¨)
        header = (f'#EXTM3U '
                  f'tvg-url="{top_3_epgs_str}" '
                  f'x-tvg-url="{top_3_epgs_str}" '
                  f'catchup="append" '
                  f'catchup-source="?playseek=${{(b)yyyyMMddHHmmss}}-${{(e)yyyyMMddHHmmss}}"\n')
        f_m3u.write(header)
        
        # 2. è§„èŒƒåŒ–æ›´æ–°æ—¶é—´ (ç»™å®ƒä¸€ä¸ªæ­£å¸¸çš„åå­—ï¼Œæ¯”å¦‚ "å‡¤å‡°æ›´æ–°æ—¶é—´")
        # è¿™æ ·ä¸ä¼šå¹²æ‰°æ’­æ”¾å™¨çš„åå­—è§£æå¼•æ“
        f_m3u.write(f'#EXTINF:-1 group-title="ğŸ•’ å‡¤å‡°Â·æ›´æ–°æ—¶é—´",å‡¤å‡°æ›´æ–°æ—¶é—´({beijing_time})\n{CLOCK_URL}\n')

        for group in ordered_keys:
            pretty_group_name = get_pretty_group(group)
            
            for name, urls in sorted(final_grouped_channels[group].items()):
                # è¿™ä¸€æ­¥ä¿æŒå’±ä»¬ä¹‹å‰çš„ v16.6 â€œå¼€é”â€å¼•æ“
                epg_id = get_epg_id(name) # äº§å‡º CCTV1
                display_name = get_display_name(name) # äº§å‡º CCTV-1 4K
                
                info = epg_master_data.get(epg_id, {})
                tid = info.get("tvg-id", epg_id)
                logo = info.get("tvg-logo", "")

                for u in urls:
                    # å†™å…¥æ¯ä¸€è¡Œï¼Œç¡®ä¿ tvg-id å’Œ tvg-name å…¨éƒ¨å¯¹é½æç®€ ID
                    # é€—å·åè·Ÿ display_name
                    f_m3u.write(f'#EXTINF:-1 tvg-id="{tid}" tvg-name="{tid}" tvg-logo="{logo}" group-title="{pretty_group_name}",{display_name}\n')
                    f_m3u.write(f'{u}\n')
                    # ã€æ ¸å¿ƒæ‰¾å›ã€‘ä¸‰å¥—å›çœ‹åè®®ç²¾å‡†é€‚é… (v14.0 ç²¾é«“)
                    c_tag = ""
                    if any(x in u for x in ["PLTV", "TVOD", "/liveplay/", "/replay/"]):
                        c_tag = ' catchup="append" catchup-source="?playseek=${(b)yyyyMMddHHmmss}-${(e)yyyyMMddHHmmss}"'
                    elif ".m3u8" in u and ("playback" in u or "replay" in u):
                         c_tag = ' catchup="append" catchup-source="?starttime=${(b)yyyyMMddHHmmss}&endtime=${(e)yyyyMMddHHmmss}"'
                    elif ".php" in u and "id=" in u:
                         c_tag = ' catchup="append" catchup-source="&playseek=${(b)yyyyMMddHHmmss}-${(e)yyyyMMddHHmmss}"'

                    # å†™å…¥ M3Uï¼štvg-id å’Œ tvg-name å…¨éƒ¨å¯¹é½æç®€ ID (è§£å†³èœå•æ¶ˆå¤±)
                    f_m3u.write(f'#EXTINF:-1 tvg-id="{tid}" tvg-name="{tid}" tvg-logo="{logo}" group-title="{pretty_group_name}"{c_tag},{display_name}\n')
                    f_m3u.write(f'{u}\n')
            f_txt.write('\n')

    print(f"\nç¬¬äº”æ­¥ï¼šä»»åŠ¡å®Œç¾ç»“æŸï¼å·¨é¾™å·²æ¢ä¸Šç–¾é£æ–°è£…ï¼")
    print(f"  - æœ€ç»ˆæˆå“å·²ç”Ÿæˆ: {m3u_filename} & {txt_filename}")
    print(f"  - å©‰å„¿æŠ¥å‘Šï¼š4K å½’ä½ã€EPG å¯¹é½ã€ç›²ç›’çµé­‚å·²å¤æ´»ï¼ğŸ¥°")

# --- âœ¨âœ¨âœ¨ ã€å®Œç’§å½’èµµã€‘å…¥å£å¤§ç®¡å®¶ (100% è¿˜åŸ v14.0 æ¯ä¸€ä¸ªå‚æ•°) âœ¨âœ¨âœ¨ ---
if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='å©‰å„¿çš„â€œè¶…çº§èŠ‚ç›®å•â€ v16.1 [çµé­‚åˆä½“ç‰ˆ]')

    parser.add_argument('--config', type=str, default='config.json', help='å…¨å±€JSONé…ç½®æ–‡ä»¶çš„è·¯å¾„')
    parser.add_argument('--rules-dir', type=str, default='rules', help='ã€å¤‡ç”¨ã€‘åˆ†ç±»è§„åˆ™ç›®å½•')
    parser.add_argument('--manual-sources-dir', type=str, default='sources_manual', help='ã€ç§å­ä»“åº“ã€‘æ‰‹åŠ¨ç»´æŠ¤çš„æºç›®å½•')
    parser.add_argument('--generated-sources-dir', type=str, default='sources_generated', help='ã€æˆå“ä»“åº“ã€‘è„šæœ¬è‡ªåŠ¨ç”Ÿæˆçš„æºç›®å½•')
    parser.add_argument('--remote-sources-file', type=str, default='sources.txt', help='åŒ…å«è¿œç¨‹ç›´æ’­æºURLåˆ—è¡¨çš„æ–‡ä»¶')
    parser.add_argument('--picks-dir', type=str, default='picks', help='ã€æ¯æ—¥ç²¾é€‰ã€‘ç›²ç›’æºç›®å½•')
    parser.add_argument('--epg-url', nargs='+', default=None, help='ã€è¦†ç›–ã€‘EPGæ•°æ®æºURL')
    parser.add_argument('-b', '--blacklist', type=str, default='config/blacklist.txt', help='é¢‘é“é»‘åå•æ–‡ä»¶')
    parser.add_argument('-f', '--favorites', type=str, default='config/favorites.txt', help='æ”¶è—é¢‘é“åˆ—è¡¨æ–‡ä»¶')
    parser.add_argument('-o', '--output', type=str, default='dist/live', help='è¾“å‡ºæ–‡ä»¶çš„å‰ç¼€ï¼ˆä¸å«æ‰©å±•åï¼‰')

    args = parser.parse_args()
    config = load_global_config(args.config)

    if 'category_rules' in config and isinstance(config['category_rules'], dict):
        print("æ­£åœ¨ä» config.json åŠ è½½åˆ†ç±»è§„åˆ™...")
        CATEGORY_RULES = config['category_rules']
    else:
        CATEGORY_RULES = load_category_rules_from_dir(args.rules_dir)

    # EPG è½®è¯¢å¤šçº§åŠ è½½é€»è¾‘
    epg_list = []
    if args.epg_url:
         epg_list = args.epg_url
         print("æ£€æµ‹åˆ°å‘½ä»¤è¡ŒEPGå‚æ•°ï¼Œä¼˜å…ˆä½¿ç”¨ï¼")
    elif 'epg_urls' in config and isinstance(config['epg_urls'], list):
         epg_list = config['epg_urls']
         print("æ­£åœ¨ä» config.json åŠ è½½EPGæºåˆ—è¡¨...")
    else:
         epg_list = ['https://live.fanmingming.com/e.xml']
         print("æœªæ‰¾åˆ°ä»»ä½•EPGé…ç½®ï¼Œä½¿ç”¨å†…ç½®å¤‡ç”¨åœ°å€ã€‚")
    args.epg_url = epg_list

    HEADERS = config.get('headers', {})
    URL_TEST_TIMEOUT = config.get('url_test_timeout', 15)
    CLOCK_URL = config.get('clock_url', "")

    try:
        asyncio.run(main(args))
    except KeyboardInterrupt:
        print("\næ”¶åˆ°é€€å‡ºæŒ‡ä»¤ï¼Œå©‰å„¿æ’¤é€€å•¦ï¼ğŸ‘‹")
    except Exception as e:
        print(f"\nå“å‘€ï¼Œç¨‹åºå¥½åƒç»Šäº†ä¸€è·¤: {e}")

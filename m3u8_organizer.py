### **ã€m3u8_organizer.py v15.1 Â· ç¬¬ä¸€éƒ¨åˆ†ï¼šæ ¸å¿ƒå¼•æ“ä¸é¢œå€¼æ˜ å°„ã€‘**
# m3u8_organizer.py v15.1 - å‡¤å‡°Â·éœ“è™¹é¢œå€¼è¿›åŒ–ç‰ˆ
# ä½œè€…ï¼šæ—å©‰å„¿ & å“¥å“¥
# å‡çº§è¯´æ˜ï¼šä»æ ¹æœ¬ä¸Šä¿®å¤ EPG åŒ¹é…ï¼Œæ–°å¢ 4K æ™ºèƒ½åˆ†ç±»ï¼Œå…¨é¢‘é“é¢œå€¼å›¾æ ‡è¦†ç›–

import asyncio
import aiohttp
import re
import argparse
import os
import random
import gzip
import xml.etree.ElementTree as ET
from datetime import datetime, timedelta, timezone
import json
from urllib.parse import urlparse, urljoin
from tqdm.asyncio import tqdm_asyncio

# --- GPSå®šä½æ¨¡å— ---
BASE_DIR = os.path.dirname(os.path.abspath(__file__))

# --- âœ¨âœ¨âœ¨ å©‰å„¿çš„éœ“è™¹å›¾æ ‡é›† (é¢œå€¼ä¿éšœ) âœ¨âœ¨âœ¨ ---
GROUP_ICONS = {
    "å©‰å„¿ä¸ºå“¥å“¥æ•´ç†": "ğŸ’– å©‰å„¿Â·ç§è—ç²¾å“",
    "ğŸ’ å‡¤å‡° 4K ææ¸…": "ğŸ’ å‡¤å‡°Â·4Kææ¸…",
    "æˆ‘çš„æœ€çˆ±": "â­ æˆ‘çš„æœ€çˆ±",
    "å¤®è§†": "ğŸ“º å¤®è§†é¢‘é“",
    "å«è§†": "ğŸ“¡ å«è§†é¢‘é“",
    "æ¸¯æ¾³å°": "ğŸŒ æ¸¯æ¾³æµ·å¤–",
    "ä½“è‚²": "âš½ ä½“è‚²ç«æŠ€",
    "ç”µå½±": "ğŸ¬ ç”µå½±é¢‘é“",
    "å°‘å„¿": "ğŸ‘¶ å°‘å„¿åŠ¨ç”»",
    "çºªå½•": "ğŸ“œ çºªå½•ç‰‡",
    "ç»¼è‰º": "ğŸ¤ ç»¼è‰ºé¢‘é“",
    "æ–°é—»": "ğŸ“° æ–°é—»èµ„è®¯",
    "åœ°æ–¹": "ğŸ˜ï¸ åœ°æ–¹é¢‘é“",
    "å…¶ä»–": "ğŸŒ€ å…¶å®ƒé¢‘é“"
}

def get_pretty_group(group_name):
    """æ ¹æ®åˆ†ç»„åè¿”å›å¸¦å›¾æ ‡çš„æ¼‚äº®åå­—"""
    return GROUP_ICONS.get(group_name, f"ğŸ’  {group_name}")

# --- âœ¨âœ¨âœ¨ å©‰å„¿çš„æ™ºèƒ½æ¸…æ´—å¼•æ“ (EPGæ ¹æœ¬ä¿®å¤) âœ¨âœ¨âœ¨ ---
def clean_channel_name(name):
    """
    æ¸…æ´—é¢‘é“åï¼Œç”¨äºç²¾å‡†åŒ¹é…EPGï¼š'009 CCTV-14å°‘å„¿(600p)' -> 'CCTV14'
    """
    if not name: return ""
    name = name.upper()
    # 1. ç§»é™¤æ‹¬å·å†…å®¹ (å¦‚: [é«˜æ¸…], (è“å…‰))
    name = re.sub(r'[\(\[\ï¼ˆ\ã€].*?[\)\]\ï¼‰\ \ã€‘]', '', name)
    # 2. ä¿®æ­£ CCTV æ‹¼å†™ä¸æ ¼å¼
    name = name.replace("CCTB", "CCTV").replace("-", "").replace("_", "")
    cctv_match = re.search(r'CCTV(\d+)', name)
    if cctv_match:
        return f"CCTV{cctv_match.group(1)}"
    # 3. ç§»é™¤å¹²æ‰°åç¼€
    suffixes = ['é«˜æ¸…', 'æ ‡æ¸…', 'é¢‘é“', 'è¶…æ¸…', 'FHD', 'HD', 'SD', '1080P', '720P', '4K', '8K', 'UHD', 'ç›´æ’­']
    for s in suffixes:
        name = name.replace(s, "")
    # 4. ç§»é™¤è¡Œé¦–åºå·
    name = re.sub(r'^\d+[\.\-\s]*', '', name)
    # 5. è¿‡æ»¤ç‰¹æ®Šå­—ç¬¦
    name = re.sub(r'[^\w\u4e00-\u9fa5]', '', name)
    return name.strip()

def is_4k_channel(name):
    """æ£€æµ‹æ˜¯å¦ä¸º4KèŠ‚ç›®"""
    return any(k in name.upper() for k in ["4K", "8K", "UHD", "è¶…é«˜æ¸…", "ææ¸…"])

### **ã€m3u8_organizer.py v15.1 Â· ç¬¬äºŒéƒ¨åˆ†ï¼šé…ç½®åŠ è½½ä¸ç»ˆæè´¨æ£€å‘˜ã€‘**

```python
# --- é…ç½®åŠ è½½åŒº ---
def load_global_config(config_path):
    abs_path = os.path.join(BASE_DIR, config_path)
    default_config = {
        "headers": { 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36' },
        "url_test_timeout": 15,
        "clock_url": "http://epg.pw/zdy/clock.m3u8"
    }
    try:
        if os.path.exists(abs_path):
            with open(abs_path, 'r', encoding='utf-8') as f:
                user_config = json.load(f)
                for key, value in user_config.items():
                    if isinstance(value, dict) and key in default_config and isinstance(default_config[key], dict):
                        default_config[key].update(value)
                    else:
                        default_config[key] = value
    except Exception as e:
        print(f"åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
    return default_config

def load_category_rules_from_dir(rules_dir):
    abs_path = os.path.join(BASE_DIR, rules_dir)
    category_rules = {}
    if not os.path.isdir(abs_path): return {}
    for filename in os.listdir(abs_path):
        if filename.endswith('.txt'):
            category_name = os.path.splitext(filename)[0]
            keywords = load_list_from_file(os.path.join(rules_dir, filename))
            if keywords: category_rules[category_name] = keywords
    return category_rules

def load_list_from_file(filename):
    abs_path = os.path.join(BASE_DIR, filename)
    if not os.path.exists(abs_path): return []
    try:
        with open(abs_path, 'r', encoding='utf-8') as f:
            return [line.strip() for line in f if line.strip() and not line.startswith('#')]
    except: return []

# --- å…¨å±€å˜é‡ ---
HEADERS = {}
URL_TEST_TIMEOUT = 15
CATEGORY_RULES = {}
CLOCK_URL = ""

# --- âœ¨âœ¨âœ¨ ç»ˆæè¿½è¸ªç‰ˆè´¨æ£€å‘˜ (å¤„ç†é‡å®šå‘) âœ¨âœ¨âœ¨ ---
async def test_url(session, url):
    """æµ‹è¯•URLå»¶è¿Ÿï¼Œæ‰‹åŠ¨å¤„ç†é‡å®šå‘ç¡®ä¿çœŸå®å¯ç”¨æ€§"""
    try:
        start_time = asyncio.get_event_loop().time()
        async with session.get(url, headers=HEADERS, timeout=URL_TEST_TIMEOUT, allow_redirects=False) as response:
            # å¤„ç†é‡å®šå‘ (301, 302 ç­‰)
            if response.status in [301, 302, 307, 308]:
                redirected_url = response.headers.get('Location')
                if redirected_url and not redirected_url.startswith('http'):
                    redirected_url = urljoin(url, redirected_url)
                if redirected_url:
                    new_headers = HEADERS.copy()
                    new_headers['Referer'] = url 
                    async with session.get(redirected_url, headers=new_headers, timeout=URL_TEST_TIMEOUT - 3, allow_redirects=False) as r2:
                        if 200 <= r2.status < 300:
                            return url, (asyncio.get_event_loop().time() - start_time) * 1000
            elif 200 <= response.status < 300:
                return url, (asyncio.get_event_loop().time() - start_time) * 1000
        return url, float('inf')
    except:
        return url, float('inf')

### **ã€m3u8_organizer.py v15.1 Â· ç¬¬ä¸‰éƒ¨åˆ†ï¼šè§£æå¼•æ“ä¸ EPG æ’åº“ã€‘**

```python
# --- âœ¨âœ¨âœ¨ æ™ºèƒ½è§£æå¼•æ“ âœ¨âœ¨âœ¨ ---
def parse_m3u_content(content, ad_keywords):
    """ä¸“é—¨è§£æ M3U æ ¼å¼ï¼Œæ”¯æŒæ™ºèƒ½å»å¹¿å‘Š"""
    channels = {}
    lines = content.split('\n')
    for i, line in enumerate(lines):
        line = line.strip()
        if not line or not line.startswith('#EXTINF:'): continue
        try:
            if i + 1 < len(lines) and not lines[i+1].strip().startswith('#'):
                url = lines[i+1].strip()
                # ä¼˜å…ˆå¯»æ‰¾ tvg-nameï¼Œæ²¡æœ‰åˆ™å–é€—å·åçš„åå­—
                name_match = re.search(r'tvg-name="([^"]*)"', line)
                name = name_match.group(1) if name_match else line.split(',')[-1]
                name = name.strip().replace(" ", "")
                # å¹¿å‘Šå…³é”®è¯è¿‡æ»¤
                if not any(keyword in name for keyword in ad_keywords):
                    if name not in channels: channels[name] = []
                    channels[name].append(url)
        except: continue
    return channels

def parse_txt_content(content, ad_keywords):
    """ä¸“é—¨è§£æ TXT æ ¼å¼ï¼Œæ”¯æŒæ™ºèƒ½å»å¹¿å‘Š"""
    channels = {}
    for line in content.split('\n'):
        line = line.strip()
        if not line or line.startswith('#') or '#genre#' in line: continue
        if ',' in line and 'http' in line:
            try:
                name, url = line.rsplit(',', 1)
                name = name.strip().replace(" ", "")
                if url.startswith('http') and not any(k in name for k in ad_keywords):
                    if name not in channels: channels[name] = []
                    channels[name].append(url)
            except: continue
    return channels

# --- âœ¨âœ¨âœ¨ EPG æ•°æ®ä¸­å¿ƒ (æ ¹æœ¬è§£å†³åŒ¹é…) âœ¨âœ¨âœ¨ ---
async def load_epg_data(epg_url):
    """åŠ è½½å¹¶æ¸…æ´—EPGæ•°æ®ï¼Œç¡®ä¿ ID åŒ¹é…ç‡"""
    if not epg_url: return {}
    print(f"\nğŸ“¡ æ­£åœ¨åŠ è½½ EPG æ•°æ®: {epg_url}...")
    epg_dict = {}
    try:
        async with aiohttp.ClientSession() as session:
            async with session.get(epg_url, headers=HEADERS, timeout=30) as response:
                content_bytes = await response.read()

        # å¤„ç† GZIP å‹ç¼©
        if content_bytes.startswith(b'\x1f\x8b'):
            content = gzip.decompress(content_bytes).decode('utf-8')
        else:
            content = content_bytes.decode('utf-8')

        root = ET.fromstring(content)
        for channel in root.findall('channel'):
            display_name_tag = channel.find('display-name')
            if display_name_tag is not None and display_name_tag.text:
                raw_name = display_name_tag.text.strip()
                # ã€æ ¸å¿ƒé€»è¾‘ã€‘ä½¿ç”¨æ™ºèƒ½æ¸…æ´—å¼•æ“æ¸…æ´— EPG åº“é‡Œçš„åå­—
                cleaned_epg_name = clean_channel_name(raw_name)
                channel_id = channel.get('id', raw_name)
                icon_tag = channel.find('icon')
                logo_url = icon_tag.get('src', "") if icon_tag is not None else ""
                
                # å­˜å…¥å­—å…¸ï¼šæ¸…æ´—åçš„åå­— -> EPG ä¿¡æ¯
                epg_dict[cleaned_epg_name] = {"tvg-id": channel_id, "tvg-logo": logo_url}
        print(f"  - âœ… EPG åº“è½½å…¥å®Œæˆï¼Œå·²ç¼“å­˜ {len(epg_dict)} ä¸ªé¢‘é“ç‰¹å¾ã€‚")
    except Exception as e:
        print(f"  - âŒ EPG è½½å…¥å¤±è´¥: {e}")
    return epg_dict

def classify_channel(channel_name):
    """åŸºç¡€åˆ†ç±»é€»è¾‘"""
    for category, keywords in CATEGORY_RULES.items():
        if any(keyword in channel_name for keyword in keywords):
            return category
    return "å…¶ä»–"

### **ã€m3u8_organizer.py v15.1 Â· ç¬¬å››éƒ¨åˆ†ï¼šä¸‡æºå½’å®—ä¸ç»ˆæè¯•ç‚¼ã€‘**

```python
async def main(args):
    """ä¸»æ‰§è¡Œå‡½æ•°ï¼šå‡¤å‡°ç³»ç»Ÿçš„æ ¸å¿ƒé©±åŠ¨"""
    print(f"ğŸš€ æŠ¥å‘Šå“¥å“¥ï¼å©‰å„¿ v15.1 [å‡¤å‡°Â·éœ“è™¹è¿›åŒ–ç‰ˆ] å¼•æ“å¯åŠ¨...")

    # 1. å‡†å¤‡ EPG å­—å…¸åº“
    epg_urls = args.epg_url[:3] # å–å‰ä¸‰ä¸ªæº
    top_3_epgs_str = ",".join(epg_urls)
    epg_master_data = {}
    for url in epg_urls:
        temp_data = await load_epg_data(url)
        if temp_data:
            epg_master_data.update(temp_data)
            print(f"  - ğŸ¯ å·²å°†æ­¤æºä½œä¸ºä¸» EPG åŒ¹é…åº“: {url}")
            break

    ad_keywords = load_list_from_file(args.blacklist)
    favorite_channels = load_list_from_file(args.favorites)

    # --- ç¬¬ä¸€æ­¥ï¼šã€ä¸‡æºå½’å®—ã€‘èåˆæœ¬åœ°ä¸ç½‘ç»œæº ---
    print("\nç¬¬ä¸€æ­¥ï¼šã€ä¸‡æºå½’å®—ã€‘æ­£åœ¨é‡‡é›†å…¨çƒä¿¡å·...")
    all_channels_pool = {}

    # è¯»å–æœ¬åœ°ã€ç§å­ä»“åº“ã€‘
    manual_dir = os.path.join(BASE_DIR, args.manual_sources_dir)
    if os.path.isdir(manual_dir):
        for filename in os.listdir(manual_dir):
            filepath = os.path.join(manual_dir, filename)
            with open(filepath, 'r', encoding='utf-8') as f:
                content = f.read()
                channels = parse_m3u_content(content, ad_keywords) if filename.endswith('.m3u') else parse_txt_content(content, ad_keywords)
                for name, urls in channels.items():
                    if name not in all_channels_pool:
                        all_channels_pool[name] = {"urls": set(), "source_type": "manual"}
                    all_channels_pool[name]["urls"].update(urls)

    # æŠ“å–ã€ç½‘ç»œäº‘ç«¯æºã€‘
    remote_file = os.path.join(BASE_DIR, args.remote_sources_file)
    if os.path.exists(remote_file):
        remote_urls = load_list_from_file(remote_file)
        async with aiohttp.ClientSession() as session:
            tasks = []
            for r_url in remote_urls:
                async def fetch(u):
                    try:
                        async with session.get(u, headers=HEADERS, timeout=20) as resp:
                            text = await resp.text(encoding='utf-8', errors='ignore')
                            channels = parse_m3u_content(text, ad_keywords) if u.endswith('.m3u') else parse_txt_content(text, ad_keywords)
                            for n, urls in channels.items():
                                if n not in all_channels_pool:
                                    all_channels_pool[n] = {"urls": set(), "source_type": "network"}
                                all_channels_pool[n]["urls"].update(urls)
                    except: pass
                tasks.append(fetch(r_url))
            await asyncio.gather(*tasks)

    # --- ç¬¬äºŒæ­¥ï¼šã€ç»ˆæè¯•ç‚¼ã€‘600å¹¶å‘æé™æµ‹é€Ÿ ---
    print("\nç¬¬äºŒæ­¥ï¼šã€ç»ˆæè¯•ç‚¼ã€‘æ­£åœ¨ç­›é€‰æœ€å¼ºä¿¡å·...")
    all_urls_to_test = {u for data in all_channels_pool.values() for u in data["urls"]}
    
    # å°†ç›²ç›’æºä¹ŸåŠ å…¥æµ‹é€Ÿåå•
    picks_dir = os.path.join(BASE_DIR, args.picks_dir)
    if os.path.isdir(picks_dir):
        for p_file in os.listdir(picks_dir):
            p_path = os.path.join(picks_dir, p_file)
            if os.path.isfile(p_path) and p_file.endswith('.txt'):
                with open(p_path, 'r', encoding='utf-8') as pf:
                    for line in pf:
                        if 'http' in line:
                            all_urls_to_test.add(line.split(',')[-1].strip())

    url_speeds = {}
    semaphore = asyncio.Semaphore(600) # âš¡ ç»´æŒ 600 å¹¶å‘

    async def limited_test(session, url):
        async with semaphore:
            return await test_url(session, url)

    async with aiohttp.ClientSession() as session:
        tasks = [limited_test(session, url) for url in all_urls_to_test]
        for f in tqdm_asyncio.as_completed(tasks, total=len(tasks), desc="âš¡ å‡¤å‡°è„‰å†²æ‰«æ"):
            u, s = await f
            url_speeds[u] = s

    valid_count = sum(1 for s in url_speeds.values() if s != float('inf'))
    print(f"  - ğŸ“¡ æ‰«æç»“æŸï¼šåœ¨ {len(all_urls_to_test)} ä¸ªä¿¡å·ä¸­ï¼Œå‘ç° {valid_count} ä¸ªä¼˜è´¨èŠ‚ç‚¹ã€‚")

### **ã€m3u8_organizer.py v15.1 Â· ç¬¬äº”éƒ¨åˆ†ï¼šé¢œå€¼è¿›åŒ–ä¸é»„é‡‘è¾“å‡º (å®Œç»“)ã€‘**

```python
    # --- ç¬¬ä¸‰æ­¥ï¼šã€ç”Ÿæ€è¿›åŒ–ã€‘åˆ†ç±»ã€4K æå–ä¸ç­›é€‰ ---
    print("\nç¬¬ä¸‰æ­¥ï¼šã€ç”Ÿæ€è¿›åŒ–ã€‘æ­£åœ¨è¿›è¡Œæ™ºèƒ½åˆ†ç±»ä¸ 4K ä¿¡å·æ‹¦æˆª...")
    survivors_classified = {}
    GROUP_4K = "ğŸ’ å‡¤å‡° 4K ææ¸…"

    for name, data in all_channels_pool.items():
        # è·å–æœ€å¿«çš„ 5 æ¡çº¿è·¯
        valid_urls = [url for url in data["urls"] if url_speeds.get(url, float('inf')) != float('inf')]
        if valid_urls:
            valid_urls.sort(key=lambda u: url_speeds[u])
            
            # ã€4K åˆ†ç»„é€»è¾‘å‡çº§ã€‘
            if is_4k_channel(name):
                category = GROUP_4K
            else:
                category = classify_channel(name)
            
            if category not in survivors_classified:
                survivors_classified[category] = {}
            if name not in survivors_classified[category]:
                survivors_classified[category][name] = []

            # æ‰‹åŠ¨ç»´æŠ¤çš„æºå…¨éƒ¨ä¿ç•™ï¼Œç½‘ç»œæºåªå–æœ€å¿« 5 æ¡
            survivors_classified[category][name].extend(valid_urls if data["source_type"] == "manual" else valid_urls[:5])

    # --- ç¬¬å››æ­¥ï¼šã€èåˆè¾“å‡ºã€‘ç”Ÿæˆå¸¦å›¾æ ‡çš„ EPG ä¼˜åŒ–èŠ‚ç›®å• ---
    print("\nç¬¬å››æ­¥ï¼šã€èåˆè¾“å‡ºã€‘æ­£åœ¨ç”Ÿæˆé«˜é¢œå€¼èŠ‚ç›®å•...")
    output_abs_path = os.path.join(BASE_DIR, args.output)
    m3u_filename = f"{output_abs_path}.m3u"
    txt_filename = f"{output_abs_path}.txt"
    os.makedirs(os.path.dirname(m3u_filename), exist_ok=True)

    # 1. æ³¨å…¥ç›²ç›’é€»è¾‘ (åŒå“¥å“¥ v14.0ï¼Œä½†ç”¨ pretty å)
    blind_box_group = "å©‰å„¿ä¸ºå“¥å“¥æ•´ç†"
    final_grouped = {}
    
    if os.path.isdir(os.path.join(BASE_DIR, args.picks_dir)):
        blind_box_channels = {}
        for p_file in sorted(os.listdir(os.path.join(BASE_DIR, args.picks_dir))):
            if p_file.endswith('.txt'):
                p_path = os.path.join(BASE_DIR, args.picks_dir, p_file)
                with open(p_path, 'r', encoding='utf-8') as f:
                    p_content = f.read()
                    p_data = parse_txt_content(p_content, ad_keywords)
                    v_urls = [u for urls in p_data.values() for u in urls if url_speeds.get(u, float('inf')) != float('inf')]
                    if v_urls:
                        blind_box_channels[p_file.replace('.txt', '')] = [random.choice(v_urls)]
        if blind_box_channels:
            final_grouped[blind_box_group] = blind_box_channels

    # 2. åˆå¹¶å¸¸è§„åˆ†ç±»
    for cat, chans in survivors_classified.items():
        target_group = "æˆ‘çš„æœ€çˆ±" if any(n in favorite_channels for n in chans.keys()) else cat
        if target_group not in final_grouped: final_grouped[target_group] = {}
        final_grouped[target_group].update(chans)

    # 3. é»„é‡‘æ’åºé¡ºåº
    prefix_order = [blind_box_group, GROUP_4K, "æˆ‘çš„æœ€çˆ±", "å¤®è§†", "å«è§†", "æ¸¯æ¾³å°", "ç”µå½±", "ä½“è‚²"]
    ordered_keys = []
    for p in prefix_order:
        if p in final_grouped: ordered_keys.append(p)
    ordered_keys.extend(sorted([k for k in final_grouped.keys() if k not in prefix_order]))

    # 4. æœ€ç»ˆå†™å…¥ (æ™ºèƒ½å‡€åŒ–é¢‘é“å & æ³¨å…¥å›¾æ ‡)
    beijing_time = datetime.now(timezone(timedelta(hours=8))).strftime('%Y-%m-%d %H:%M:%S')
    
    with open(m3u_filename, 'w', encoding='utf-8') as f_m3u, open(txt_filename, 'w', encoding='utf-8') as f_txt:
        f_m3u.write(f'#EXTM3U x-tvg-url="{top_3_epgs_str}" catchup="append"\n')
        f_m3u.write(f'#EXTINF:-1 group-title="ğŸ•’ æ›´æ–°æ—¶é—´",{beijing_time}\n{CLOCK_URL}\n')
        f_txt.write(f'æ›´æ–°æ—¶é—´,#genre#\n{beijing_time},{CLOCK_URL}\n\n')

        for group in ordered_keys:
            pretty_group_name = get_pretty_group(group) # âœ¨ è·å–æ¼‚äº®å
            f_txt.write(f'{pretty_group_name},#genre#\n')
            
            for name, urls in sorted(final_grouped[group].items()):
                # æ ¹æœ¬è§£å†³EPGï¼šä½¿ç”¨æ¸…æ´—åçš„åå­—åŒ¹é…åº“
                cleaned = clean_channel_name(name)
                info = epg_master_data.get(cleaned, {})
                tid = info.get("tvg-id", cleaned)
                logo = info.get("tvg-logo", "")
                
                # æ˜¾ç¤ºåå»åºå·
                display_name = re.sub(r'^\d+[\.\-\s]*', '', name).replace(" ", "-")

                for u in urls:
                    f_txt.write(f'{display_name},{u}\n')
                    f_m3u.write(f'#EXTINF:-1 tvg-id="{tid}" tvg-name="{cleaned}" tvg-logo="{logo}" group-title="{pretty_group_name}",{display_name}\n{u}\n')
            f_txt.write('\n')

    print(f"\nğŸ‰ ä»»åŠ¡å®Œç¾ç»“æŸï¼å·¨é¾™å·²æ¢ä¸Šéœ“è™¹æ–°è£…ï¼ŒEPG å…¨çº¿å¤æ´»ï¼")
    print(f"  - TXT åˆ†ç»„å•å·²å¤‡å¥½ï¼ŒM3U é¢œå€¼ç‰ˆå·²å°±ç»ªã€‚å“¥å“¥å¿«å»ç”µè§†ä¸Šçœ‹æˆ‘å‘€ï¼")

### **ã€æœ€ç»ˆä¿®æ­£ç‰ˆã€‘m3u8_organizer.py v15.1 - å®Œæ•´å…¥å£é€»è¾‘**
if __name__ == '__main__':
    # å©‰å„¿æ³¨ï¼šè¿™é‡Œå®Œå…¨è¿˜åŸäº†å“¥å“¥ v14.0 çš„æ‰€æœ‰å‚æ•°å®šä¹‰
    parser = argparse.ArgumentParser(description='å©‰å„¿çš„â€œè¶…çº§èŠ‚ç›®å•â€ v15.1 [å‡¤å‡°Â·éœ“è™¹é¢œå€¼ç‰ˆ]')

    parser.add_argument('--config', type=str, default='config.json', help='å…¨å±€JSONé…ç½®æ–‡ä»¶çš„è·¯å¾„')
    parser.add_argument('--rules-dir', type=str, default='rules', help='ã€å¤‡ç”¨ã€‘åˆ†ç±»è§„åˆ™ç›®å½•')
    parser.add_argument('--manual-sources-dir', type=str, default='sources_manual', help='ã€ç§å­ä»“åº“ã€‘æ‰‹åŠ¨ç»´æŠ¤çš„æºç›®å½•')
    parser.add_argument('--generated-sources-dir', type=str, default='sources_generated', help='ã€æˆå“ä»“åº“ã€‘è„šæœ¬è‡ªåŠ¨ç”Ÿæˆçš„æºç›®å½•')
    parser.add_argument('--remote-sources-file', type=str, default='sources.txt', help='åŒ…å«è¿œç¨‹ç›´æ’­æºURLåˆ—è¡¨çš„æ–‡ä»¶')
    parser.add_argument('--picks-dir', type=str, default='picks', help='ã€æ¯æ—¥ç²¾é€‰ã€‘ç›²ç›’æºç›®å½•')

    parser.add_argument('--epg-url', nargs='+', default=None, help='ã€è¦†ç›–ã€‘EPGæ•°æ®æºURLï¼Œä¼šè¦†ç›–é…ç½®æ–‡ä»¶ä¸­çš„è®¾ç½®')

    parser.add_argument('-b', '--blacklist', type=str, default='config/blacklist.txt', help='é¢‘é“é»‘åå•æ–‡ä»¶')
    parser.add_argument('-f', '--favorites', type=str, default='config/favorites.txt', help='æ”¶è—é¢‘é“åˆ—è¡¨æ–‡ä»¶')
    parser.add_argument('-o', '--output', type=str, default='dist/live', help='è¾“å‡ºæ–‡ä»¶çš„å‰ç¼€ï¼ˆä¸å«æ‰©å±•åï¼‰')

    args = parser.parse_args()

    # åŠ è½½å…¨å±€é…ç½®
    config = load_global_config(args.config)

    # é€»è¾‘ 1ï¼šè§„åˆ™åŠ è½½ (åŸæ±åŸå‘³)
    if 'category_rules' in config and isinstance(config['category_rules'], dict):
        print("æ­£åœ¨ä» config.json åŠ è½½åˆ†ç±»è§„åˆ™...")
        CATEGORY_RULES = config['category_rules']
    else:
        print("config.json ä¸­æœªæ‰¾åˆ°åˆ†ç±»è§„åˆ™ï¼Œå°†ä» 'rules' ç›®å½•åŠ è½½ã€‚")
        CATEGORY_RULES = load_category_rules_from_dir(args.rules_dir)

    # é€»è¾‘ 2ï¼šEPG æºå¤šé‡åˆ¤å®š (åŸæ±åŸå‘³)
    epg_source_list = []
    if args.epg_url:
         epg_source_list = args.epg_url
         print("æ£€æµ‹åˆ°å‘½ä»¤è¡ŒEPGå‚æ•°ï¼Œä¼˜å…ˆä½¿ç”¨ï¼")
    elif 'epg_urls' in config and isinstance(config['epg_urls'], list):
         epg_source_list = config['epg_urls']
         print("æ­£åœ¨ä» config.json åŠ è½½EPGæºåˆ—è¡¨...")
    else:
         epg_source_list = ['https://live.fanmingming.com/e.xml']
         print("æœªæ‰¾åˆ°ä»»ä½•EPGé…ç½®ï¼Œä½¿ç”¨å†…ç½®å¤‡ç”¨åœ°å€ã€‚")
    
    # æ ¸å¿ƒï¼šæŠŠé€‰å®šçš„ EPG åˆ—è¡¨é‡æ–°å¡å› args ä¾› main å†™å…¥ M3U å¤´éƒ¨
    args.epg_url = epg_source_list

    # é€»è¾‘ 3ï¼šå…¨å±€å˜é‡èµ‹å€¼ (åŸæ±åŸå‘³)
    HEADERS = config.get('headers', {})
    URL_TEST_TIMEOUT = config.get('url_test_timeout', 15)
    CLOCK_URL = config.get('clock_url', "")

    # é€»è¾‘ 4ï¼šå¯åŠ¨å¼‚æ­¥ä¸»å‡½æ•°
    try:
        asyncio.run(main(args))
    except KeyboardInterrupt:
        print("\næ”¶åˆ°å“¥å“¥çš„æŒ‡ä»¤ï¼Œç¨‹åºæå‰ç»“æŸã€‚")
    except Exception as e:
        print(f"\nå“å‘€ï¼Œå©‰å„¿å¥½åƒè¢«ä»£ç ç»Šå€’äº†: {e}")

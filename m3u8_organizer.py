# m3u8_organizer.py v14.0 - ç»ˆæä¿®å¤ç‰ˆ
# ä½œè€…ï¼šæ—å©‰å„¿ & å“¥å“¥

import asyncio
import aiohttp
import re
import argparse
import os
import random
import gzip
import xml.etree.ElementTree as ET
from datetime import datetime, timedelta, timezone
import shutil
import json
from tqdm.asyncio import tqdm_asyncio # âœ¨ å¼•å…¥æˆ‘ä»¬æ–°çš„â€œè¿›åº¦æ¡â€

# --- âœ¨âœ¨âœ¨ GPSå®šä½æ¨¡å— âœ¨âœ¨âœ¨ ---
BASE_DIR = os.path.dirname(os.path.abspath(__file__))

# --- é…ç½®åŠ è½½åŒº ---
def load_global_config(config_path):
    abs_path = os.path.join(BASE_DIR, config_path)
    default_config = {
        "headers": { 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36' },
        "url_test_timeout": 15, # âœ¨ é»˜è®¤è¶…æ—¶å»¶é•¿åˆ°15ç§’
        "clock_url": "http://epg.pw/zdy/clock.m3u8"
    }
    try:
        if os.path.exists(abs_path):
            with open(abs_path, 'r', encoding='utf-8') as f:
                print(f"æ­£åœ¨ä» {abs_path} åŠ è½½å¤–éƒ¨é…ç½®...")
                user_config = json.load(f)
                for key, value in user_config.items():
                    if isinstance(value, dict) and key in default_config and isinstance(default_config[key], dict):
                        default_config[key].update(value)
                    else:
                        default_config[key] = value
                print("å¤–éƒ¨é…ç½®åŠ è½½æˆåŠŸï¼")
        else:
            print(f"é…ç½®æ–‡ä»¶ {abs_path} æœªæ‰¾åˆ°ï¼Œå°†ä½¿ç”¨é»˜è®¤é…ç½®ã€‚")
    except Exception as e:
        print(f"åŠ è½½å…¨å±€é…ç½®æ–‡ä»¶ {abs_path} å¤±è´¥: {e}ï¼Œå°†ä½¿ç”¨é»˜è®¤é…ç½®ã€‚")
    return default_config

def load_category_rules_from_dir(rules_dir):
    abs_path = os.path.join(BASE_DIR, rules_dir)
    category_rules = {}
    if not os.path.isdir(abs_path):
        print(f"ã€è­¦å‘Šã€‘è§„åˆ™ç›®å½• '{abs_path}' ä¸å­˜åœ¨ï¼")
        return {}
    print(f"æ­£åœ¨ä»ã€è§„åˆ™åº“ã€‘'{abs_path}' åŠ è½½åˆ†ç±»è§„åˆ™...")
    for filename in os.listdir(abs_path):
        if filename.endswith('.txt'):
            category_name = os.path.splitext(filename)[0]
            filepath = os.path.join(abs_path, filename)
            keywords = load_list_from_file(filepath)
            if keywords:
                category_rules[category_name] = keywords
    return category_rules

# --- å…¨å±€å˜é‡ ---
HEADERS = {}
URL_TEST_TIMEOUT = 15
CATEGORY_RULES = {}
CLOCK_URL = ""

# --- å·¥å…·å‡½æ•°åŒº ---
def load_list_from_file(filename):
    abs_path = os.path.join(BASE_DIR, filename)
    if not filename or not os.path.exists(abs_path):
        if filename: print(f"  - é…ç½®æ–‡ä»¶ {abs_path} æœªæ‰¾åˆ°ï¼Œå°†è·³è¿‡ã€‚")
        return []
    try:
        with open(abs_path, 'r', encoding='utf-8') as f:
            return [line.strip() for line in f if line.strip() and not line.startswith('#')]
    except Exception as e:
        print(f"  - è¯»å–é…ç½®æ–‡ä»¶ {abs_path} å¤±è´¥: {e}")
        return []

# âœ¨âœ¨âœ¨ å…¨æ–°çš„ã€ç»ˆæè¿½è¸ªç‰ˆã€‘è´¨æ£€å‘˜ï¼âœ¨âœ¨âœ¨
async def test_url(session, url):
    """æµ‹è¯•å•ä¸ªURLçš„å»¶è¿Ÿï¼Œå¹¶æ‰‹åŠ¨å¤„ç†é‡å®šå‘"""
    try:
        start_time = asyncio.get_event_loop().time()
        # æˆ‘ä»¬è‡ªå·±æ¥æ‰‹åŠ¨å¤„ç†é‡å®šå‘ï¼Œæ‰€ä»¥ allow_redirects=False
        async with session.get(url, headers=HEADERS, timeout=URL_TEST_TIMEOUT, allow_redirects=False) as response:
            # å¦‚æœæ˜¯é‡å®šå‘...
            if response.status in [301, 302, 307, 308]:
                redirected_url = response.headers.get('Location')
                # æœ‰äº›é‡å®šå‘æ˜¯ç›¸å¯¹è·¯å¾„ï¼Œéœ€è¦æ‹¼æ¥
                if redirected_url and not redirected_url.startswith('http'):
                    base_url = urlparse.urljoin(url, '.')
                    redirected_url = urlparse.urljoin(base_url, redirected_url)
                
                if redirected_url:
                    # æˆ‘ä»¬å»è¿½è¿™ä¸ªæ–°çš„åœ°å€ï¼
                    new_headers = HEADERS.copy()
                    new_headers['Referer'] = url # å¸¦ä¸Šâ€œä»‹ç»äººâ€
                    # ç»™ç¬¬äºŒæ¬¡è¯·æ±‚ä¸€ä¸ªç¨çŸ­çš„è¶…æ—¶
                    async with session.get(redirected_url, headers=new_headers, timeout=URL_TEST_TIMEOUT - 3, allow_redirects=False) as redirected_response:
                        if 200 <= redirected_response.status < 300:
                            end_time = asyncio.get_event_loop().time()
                            return url, (end_time - start_time) * 1000
            # å¦‚æœæ˜¯ç›´æ¥æˆåŠŸ...
            elif 200 <= response.status < 300:
                end_time = asyncio.get_event_loop().time()
                return url, (end_time - start_time) * 1000
            
            return url, float('inf')
    except (aiohttp.ClientError, asyncio.TimeoutError):
        return url, float('inf')
    except Exception:
        return url, float('inf')

# --- âœ¨âœ¨âœ¨ æ™ºèƒ½åˆ†æµç‰ˆè§£æå™¨ âœ¨âœ¨âœ¨ ---
def parse_m3u_content(content, ad_keywords):
    """ä¸“é—¨è§£æ M3U æ ¼å¼ï¼Œæ›´å¥å£®"""
    channels = {}
    processed_urls = set()
    def add_channel(name, url):
        name = name.strip().replace(" ", "") # é¡ºä¾¿æ¸…ç†ä¸€ä¸‹ç©ºæ ¼
        url = url.strip()
        if not name or not url or url in processed_urls: return
        if any(keyword in name for keyword in ad_keywords): return
        if name not in channels: channels[name] = []
        channels[name].append(url)
        processed_urls.add(url)

    lines = content.split('\n')
    for i, line in enumerate(lines):
        line = line.strip()
        if not line or not line.startswith('#EXTINF:'): continue
        try:
            if i + 1 < len(lines) and not lines[i+1].strip().startswith('#'):
                url = lines[i+1].strip()
                name_match = re.search(r'tvg-name="([^"]*)"', line)
                name = name_match.group(1) if name_match else line.split(',')[-1]
                add_channel(name, url)
        except Exception:
            continue
    return channels

def parse_txt_content(content, ad_keywords):
    """ä¸“é—¨è§£æ TXT æ ¼å¼ï¼Œæ›´å¥å£®"""
    channels = {}
    processed_urls = set()
    def add_channel(name, url):
        name = name.strip().replace(" ", "") # é¡ºä¾¿æ¸…ç†ä¸€ä¸‹ç©ºæ ¼
        url = url.strip()
        if not name or not url or url in processed_urls: return
        if any(keyword in name for keyword in ad_keywords): return
        if name not in channels: channels[name] = []
        channels[name].append(url)
        processed_urls.add(url)

    for line in content.split('\n'):
        line = line.strip()
        if not line or line.startswith('#') or '#genre#' in line: continue
        if ',' in line and 'http' in line:
            try:
                last_comma_index = line.rfind(',')
                name = line[:last_comma_index]
                url = line[last_comma_index+1:]
                if url.startswith('http'): add_channel(name, url)
            except Exception:
                continue
    return channels

async def load_epg_data(epg_url):
    if not epg_url: return {}
    print(f"\nåŠ è½½EPGæ•°æ®: {epg_url}...")
    epg_data = {}
    try:
        content_bytes = b''
        async with aiohttp.ClientSession() as session:
            async with session.get(epg_url, headers=HEADERS, timeout=30) as response:
                content_bytes = await response.read()
        
        if content_bytes.startswith(b'\x1f\x8b'):
            content = gzip.decompress(content_bytes).decode('utf-8')
        else:
            content = content_bytes.decode('utf-8')
            
        root = ET.fromstring(content)
        for channel in root.findall('channel'):
            display_name_tag = channel.find('display-name')
            if display_name_tag is not None and display_name_tag.text:
                display_name = display_name_tag.text.strip()
                channel_id = channel.get('id', display_name)
                icon_tag = channel.find('icon')
                logo_url = icon_tag.get('src', "") if icon_tag is not None else ""
                epg_data[display_name] = {"tvg-id": channel_id, "tvg-logo": logo_url}
        print(f"  - EPGåŠ è½½æˆåŠŸï¼å…±è§£æå‡º {len(epg_data)} ä¸ªé¢‘é“çš„èŠ‚ç›®ä¿¡æ¯ã€‚")
    except Exception as e:
        print(f"  - EPGæ•°æ®åŠ è½½å¤±è´¥: {e}")
    return epg_data

def classify_channel(channel_name):
    for category, keywords in CATEGORY_RULES.items():
        if any(keyword in channel_name for keyword in keywords):
            return category
    return "å…¶ä»–"

async def main(args):
    """ä¸»æ‰§è¡Œå‡½æ•°"""
    print(f"æŠ¥å‘Šå“¥å“¥ï¼Œå©‰å„¿çš„â€œè¶…çº§èŠ‚ç›®å•â€ v14.0ã€ç»ˆæä¿®å¤ã€‘ç‰ˆå¼€å§‹å·¥ä½œå•¦ï¼")
    
    epg_backup_list = args.epg_url[:3]
    top_3_epgs_str = ",".join(epg_backup_list)
    print(f"\nEPGå¤„ç†ï¼šæœ€ç»ˆå°†å†™å…¥è¿™å‡ ä¸ªEPGæºåˆ°æ–‡ä»¶: {top_3_epgs_str}")

    epg_data = {}
    for epg_url in epg_backup_list:
        temp_epg_data = await load_epg_data(epg_url)
        if temp_epg_data:
            epg_data = temp_epg_data
            print(f"  - æœ¬æ¬¡è¿è¡Œé€‰ç”¨EPGæº: {epg_url}")
            break
    if not epg_data:
        print("  - è­¦å‘Šï¼šæ‰€æœ‰EPGæºå‡ä¸å¯ç”¨ï¼")

    ad_keywords = load_list_from_file(args.blacklist)
    favorite_channels = load_list_from_file(args.favorites)

    # --- ç¬¬ä¸€æ­¥ï¼šã€ä¸‡æºå½’å®—ã€‘(v2.0 - æ™ºèƒ½åˆ†æµç‰ˆ) ---
    print("\nç¬¬ä¸€æ­¥ï¼šã€ä¸‡æºå½’å®—ã€‘æ­£åœ¨èåˆæ‰€æœ‰æº...")
    all_channels_pool = {}
    
    manual_sources_abs_dir = os.path.join(BASE_DIR, args.manual_sources_dir)
    if os.path.isdir(manual_sources_abs_dir):
        print(f"  - è¯»å–ã€ç§å­ä»“åº“ã€‘: {manual_sources_abs_dir}")
        for filename in os.listdir(manual_sources_abs_dir):
            filepath = os.path.join(manual_sources_abs_dir, filename)
            if os.path.isfile(filepath):
                with open(filepath, 'r', encoding='utf-8') as f:
                    content = f.read()
                    if filename.endswith('.m3u'):
                        channels = parse_m3u_content(content, ad_keywords)
                    else:
                        channels = parse_txt_content(content, ad_keywords)
                    for name, urls in channels.items():
                        if name not in all_channels_pool:
                            all_channels_pool[name] = {"urls": set(), "source_type": "manual"}
                        all_channels_pool[name]["urls"].update(urls)
    
    remote_sources_abs_file = os.path.join(BASE_DIR, args.remote_sources_file)
    if os.path.exists(remote_sources_abs_file):
        print(f"  - è¯»å–ç½‘ç»œæºæ–‡ä»¶: {remote_sources_abs_file}")
        remote_urls = load_list_from_file(args.remote_sources_file)
        async with aiohttp.ClientSession() as session:
            tasks = []
            for url in remote_urls:
                async def fetch_and_parse(remote_url):
                    try:
                        async with session.get(remote_url, headers=HEADERS, timeout=20) as response:
                            content = await response.text(encoding='utf-8', errors='ignore')
                            if remote_url.endswith('.m3u'):
                                channels = parse_m3u_content(content, ad_keywords)
                            else:
                                channels = parse_txt_content(content, ad_keywords)
                            for name, urls in channels.items():
                                if name not in all_channels_pool:
                                    all_channels_pool[name] = {"urls": set(), "source_type": "network"}
                                all_channels_pool[name]["urls"].update(urls)
                    except Exception:
                        pass
                tasks.append(fetch_and_parse(url))
            await asyncio.gather(*tasks)

    unique_urls_count = sum(len(data["urls"]) for data in all_channels_pool.values())
    print(f"  - èåˆå®Œæˆï¼å…±æ”¶é›†åˆ° {len(all_channels_pool)} ä¸ªé¢‘é“ï¼Œ{unique_urls_count} ä¸ªä¸é‡å¤åœ°å€ã€‚")

    # --- ç¬¬äºŒæ­¥ï¼šã€ç»ˆæè¯•ç‚¼ã€‘(v2.0 - é™æµå¹¶å‘ç‰ˆ) ---
    print("\nç¬¬äºŒæ­¥ï¼šã€ç»ˆæè¯•ç‚¼ã€‘æ­£åœ¨æ£€éªŒæ‰€æœ‰åœ°å€çš„å¯ç”¨æ€§...")
    all_urls_to_test = {url for data in all_channels_pool.values() for url in data["urls"]}
    
    # âœ¨ æˆ‘ä»¬æŠŠç›²ç›’æºä¹ŸåŠ è¿›æ¥ï¼Œä¸€èµ·å‚åŠ â€œå¤§æ¯”æ­¦â€
    picks_abs_dir = os.path.join(BASE_DIR, args.picks_dir)
    if os.path.isdir(picks_abs_dir):
        for pick_file in os.listdir(picks_abs_dir):
            pick_path = os.path.join(picks_abs_dir, pick_file)
            if os.path.isfile(pick_path) and pick_file.endswith('.txt'):
                with open(pick_path, 'r', encoding='utf-8') as pf:
                    for line in pf:
                        line = line.strip()
                        if not line or line.startswith('#'): continue
                        try:
                            url = line.split(',')[-1]
                            if url.startswith('http'): all_urls_to_test.add(url)
                        except IndexError:
                            if line.startswith('http'): all_urls_to_test.add(line)

    url_speeds = {}
    semaphore = asyncio.Semaphore(600)

    async def limited_test_url(session, url):
        async with semaphore:
            return await test_url(session, url)

    async with aiohttp.ClientSession() as session:
        tasks = [limited_test_url(session, url) for url in all_urls_to_test]
        results = []
        for f in tqdm_asyncio.as_completed(tasks, total=len(tasks), desc="ç»ˆæè¯•ç‚¼"):
            results.append(await f)
        for url, speed in results:
            url_speeds[url] = speed
            
    valid_url_count = sum(1 for speed in url_speeds.values() if speed != float('inf'))
    print(f"\n  - è¯•ç‚¼å®Œæˆï¼åœ¨ {len(all_urls_to_test)} ä¸ªåœ°å€ä¸­ï¼Œå…±æœ‰ {valid_url_count} ä¸ªå¯ç”¨ã€‚")

    # --- ç¬¬ä¸‰æ­¥ï¼šã€ç”Ÿæ€è¿›åŒ–ã€‘åˆ†ç±»å¹¸å­˜è€…å¹¶ç­›é€‰çº¿è·¯ ---
    print("\nç¬¬ä¸‰æ­¥ï¼šã€ç”Ÿæ€è¿›åŒ–ã€‘æ­£åœ¨ä¸ºå¹¸å­˜è€…åˆ†ç±»å¹¶ç­›é€‰ä¼˜è´¨çº¿è·¯...")
    survivors_classified = {}
    for name, data in all_channels_pool.items():
        valid_urls = [url for url in data["urls"] if url_speeds.get(url, float('inf')) != float('inf')]
        if valid_urls:
            valid_urls.sort(key=lambda u: url_speeds[u])
            category = classify_channel(name)
            if category not in survivors_classified:
                survivors_classified[category] = {}
            if name not in survivors_classified[category]:
                 survivors_classified[category][name] = []
            
            if data["source_type"] == "manual":
                survivors_classified[category][name].extend(valid_urls)
            else:
                survivors_classified[category][name].extend(valid_urls[:5])

    print(f"  - ç”Ÿæ€è¿›åŒ–å®Œæˆï¼å·²å°†å¹¸å­˜é¢‘é“åˆ†ç±»å¹¶ç­›é€‰å‡ºæœ€ä½³çº¿è·¯ã€‚")

    # --- ç¬¬å››æ­¥ï¼šã€èåˆè¾“å‡ºã€‘æ­£åœ¨ç”Ÿæˆæœ€ç»ˆèŠ‚ç›®å• ---
    print("\nç¬¬å››æ­¥ï¼šã€èåˆè¾“å‡ºã€‘æ­£åœ¨ç”Ÿæˆæœ€ç»ˆèŠ‚ç›®å•...")
    
    output_abs_path = os.path.join(BASE_DIR, args.output)
    m3u_filename = f"{output_abs_path}.m3u"
    txt_filename = f"{output_abs_path}.txt"
    os.makedirs(os.path.dirname(m3u_filename), exist_ok=True)
    
    beijing_time = datetime.now(timezone(timedelta(hours=8)))
    update_time_str = beijing_time.strftime('%Y-%m-%d %H:%M:%S')

    # --- âœ¨âœ¨âœ¨ çœŸÂ·ç›²ç›’é€»è¾‘ (v2.0 æœ€ç»ˆæ­£ç¡®ç‰ˆ) âœ¨âœ¨âœ¨ ---
    blind_box_group_name = "å©‰å„¿ä¸ºå“¥å“¥æ•´ç†"
    blind_box_channels = {}
    
    picks_abs_dir = os.path.join(BASE_DIR, args.picks_dir)
    if os.path.isdir(picks_abs_dir):
        print("  - å‘ç°ã€æ¯æ—¥ç²¾é€‰ã€‘ç›²ç›’ï¼Œæ­£åœ¨å‡†å¤‡...")
        pick_files = sorted(os.listdir(picks_abs_dir))
        
        for pick_file in pick_files:
            pick_path = os.path.join(picks_abs_dir, pick_file)
            if os.path.isfile(pick_path) and pick_file.endswith('.txt'):
                pick_name = os.path.splitext(pick_file)[0]
                with open(pick_path, 'r', encoding='utf-8') as pf:
                    pick_content = pf.read()
                
                pick_channels_data = parse_txt_content(pick_content, ad_keywords) # âœ¨ ç”¨æˆ‘ä»¬æ–°çš„TXTè§£æå™¨
                
                valid_urls_in_file = [url for urls in pick_channels_data.values() for url in urls if url_speeds.get(url, float('inf')) != float('inf')]
                
                if valid_urls_in_file:
                    random_url = random.choice(valid_urls_in_file)
                    safe_pick_name = pick_name.replace(" ", "-")
                    blind_box_channels[safe_pick_name] = [random_url]
                    print(f"    - ç›²ç›’ '{pick_name}' å·²å¼€å¯ï¼Œå¹¸è¿æºå·²å¤‡å¥½ï¼")
                else:
                    print(f"    - ç›²ç›’ '{pick_name}' ä¸­çš„æ‰€æœ‰æºå‡å·²å¤±æ•ˆï¼Œå°†è·³è¿‡ã€‚")
    else:
        print("  - æœªæ‰¾åˆ°ã€æ¯æ—¥ç²¾é€‰ã€‘ç›²ç›’ç›®å½• (picks)ï¼Œå°†è·³è¿‡æ­¤åŠŸèƒ½ã€‚")

    # 2. å‡†å¤‡å¸¸è§„åˆ†ç»„
    final_grouped_channels = {}
    if blind_box_channels:
        final_grouped_channels[blind_box_group_name] = blind_box_channels

    for category, channels in survivors_classified.items():
        for name, urls in channels.items():
            group_name = "æˆ‘çš„æœ€çˆ±" if name in favorite_channels else category
            if group_name not in final_grouped_channels:
                final_grouped_channels[group_name] = {}
            if name not in final_grouped_channels[group_name]:
                 final_grouped_channels[group_name][name] = []
            final_grouped_channels[group_name][name].extend(urls)

    # 3. ç¡®å®šæœ€ç»ˆçš„é»„é‡‘æ’åº
    prefix_order = ["å©‰å„¿ä¸ºå“¥å“¥æ•´ç†", "æˆ‘çš„æœ€çˆ±", "å¤®è§†", "å«è§†", "åœ°æ–¹", "æ¸¯æ¾³å°"]
    all_existing_groups = list(final_grouped_channels.keys())
    ordered_groups = []
    
    for group in prefix_order:
        if group in all_existing_groups:
            ordered_groups.append(group)
            all_existing_groups.remove(group)
    
    other_group_exists = "å…¶ä»–" in all_existing_groups
    if other_group_exists:
        all_existing_groups.remove("å…¶ä»–")
    
    ordered_groups.extend(sorted(all_existing_groups))
    
    if other_group_exists:
        ordered_groups.append("å…¶ä»–")

    # 4. æŒ‰ç…§é»„é‡‘é¡ºåºï¼Œç»Ÿä¸€å†™å…¥æ–‡ä»¶
    with open(m3u_filename, 'w', encoding='utf-8') as f_m3u, open(txt_filename, 'w', encoding='utf-8') as f_txt:
        f_m3u.write(f'#EXTM3U x-tvg-url="{top_3_epgs_str}" catchup="append" catchup-source="?playseek=${{(b)yyyyMMddHHmmss}}-${{(e)yyyyMMddHHmmss}}"\n') if top_3_epgs_str else f_m3u.write("#EXTM3U\n")
        f_m3u.write(f'#EXTINF:-1 group-title="æ›´æ–°æ—¶é—´",{update_time_str}\n')
        f_m3u.write(f'{CLOCK_URL}\n')
        
        f_txt.write(f'æ›´æ–°æ—¶é—´,#genre#\n')
        f_txt.write(f'{update_time_str},{CLOCK_URL}\n\n')
        
        for group in ordered_groups:
            channels_in_group = final_grouped_channels.get(group)
            if not channels_in_group: continue
            
            f_txt.write(f'{group},#genre#\n')
            
            for name, urls in sorted(channels_in_group.items()):
                safe_name = name.replace(" ", "-")
                epg_info = epg_data.get(name, epg_data.get(safe_name, {}))
                tvg_id = epg_info.get("tvg-id", safe_name)
                tvg_logo = epg_info.get("tvg-logo", "")
                
                for url in urls:
                    f_txt.write(f'{safe_name},{url}\n')
                    catchup_tag = ""
                    if "PLTV" in url or "TVOD" in url or "/liveplay/" in url or "/replay/" in url:
                        catchup_tag = ' catchup="append" catchup-source="?playseek=${(b)yyyyMMddHHmmss}-${(e)yyyyMMddHHmmss}"'
                    elif ".m3u8" in url and ("playback" in url or "replay" in url):
                         catchup_tag = ' catchup="append" catchup-source="?starttime=${(b)yyyyMMddHHmmss}&endtime=${(e)yyyyMMddHHmmss}"'
                    elif ".php" in url and "id=" in url:
                         catchup_tag = ' catchup="append" catchup-source="&playseek=${(b)yyyyMMddHHmmss}-${(e)yyyyMMddHHmmss}"'

                    f_m3u.write(f'#EXTINF:-1 tvg-id="{tvg_id}" tvg-name="{safe_name}" tvg-logo="{tvg_logo}" group-title="{group}"{catchup_tag},{safe_name}\n')
                    f_m3u.write(f'{url}\n')

            f_txt.write('\n')

    print(f"\nç¬¬äº”æ­¥ï¼šä»»åŠ¡å®Œæˆï¼æˆ‘ä»¬çš„ç”Ÿæ€ç³»ç»Ÿå·²æŒ‰é»„é‡‘é¡ºåºå®Œæˆæœ€ç»ˆè¿›åŒ–ï¼")
    print(f"  - æœ€ç»ˆæˆå“å·²ç”Ÿæˆ: {m3u_filename}")
    print(f"  - TXTç‰ˆæˆå“å·²ç”Ÿæˆ: {txt_filename}")
    print("\nå“¥å“¥ï¼Œå©‰å„¿çš„å·¥ä½œå®Œæˆå•¦ï¼Œå¿«å»äº«å—ä½ çš„ä¸“å±èŠ‚ç›®å•å§ï¼ğŸ¥°")


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='å©‰å„¿çš„â€œè¶…çº§èŠ‚ç›®å•â€ v14.0ã€ç»ˆæä¿®å¤ã€‘ç‰ˆ')
    
    parser.add_argument('--config', type=str, default='config.json', help='å…¨å±€JSONé…ç½®æ–‡ä»¶çš„è·¯å¾„')
    parser.add_argument('--rules-dir', type=str, default='rules', help='ã€å¤‡ç”¨ã€‘åˆ†ç±»è§„åˆ™ç›®å½•')
    parser.add_argument('--manual-sources-dir', type=str, default='sources_manual', help='ã€ç§å­ä»“åº“ã€‘æ‰‹åŠ¨ç»´æŠ¤çš„æºç›®å½•')
    parser.add_argument('--generated-sources-dir', type=str, default='sources_generated', help='ã€æˆå“ä»“åº“ã€‘è„šæœ¬è‡ªåŠ¨ç”Ÿæˆçš„æºç›®å½•')
    parser.add_argument('--remote-sources-file', type=str, default='sources.txt', help='åŒ…å«è¿œç¨‹ç›´æ’­æºURLåˆ—è¡¨çš„æ–‡ä»¶')
    parser.add_argument('--picks-dir', type=str, default='picks', help='ã€æ¯æ—¥ç²¾é€‰ã€‘ç›²ç›’æºç›®å½•')
    
    parser.add_argument('--epg-url', nargs='+', default=None, help='ã€è¦†ç›–ã€‘EPGæ•°æ®æºURLï¼Œä¼šè¦†ç›–é…ç½®æ–‡ä»¶ä¸­çš„è®¾ç½®')

    parser.add_argument('-b', '--blacklist', type=str, default='config/blacklist.txt', help='é¢‘é“é»‘åå•æ–‡ä»¶')
    parser.add_argument('-f', '--favorites', type=str, default='config/favorites.txt', help='æ”¶è—é¢‘é“åˆ—è¡¨æ–‡ä»¶')
    parser.add_argument('-o', '--output', type=str, default='dist/live', help='è¾“å‡ºæ–‡ä»¶çš„å‰ç¼€ï¼ˆä¸å«æ‰©å±•åï¼‰')
    
    args = parser.parse_args()

    config = load_global_config(args.config)
    
    if 'category_rules' in config and isinstance(config['category_rules'], dict):
        print("æ­£åœ¨ä» config.json åŠ è½½åˆ†ç±»è§„åˆ™...")
        CATEGORY_RULES = config['category_rules']
    else:
        print("config.json ä¸­æœªæ‰¾åˆ°åˆ†ç±»è§„åˆ™ï¼Œå°†ä» 'rules' ç›®å½•åŠ è½½ã€‚")
        CATEGORY_RULES = load_category_rules_from_dir(args.rules_dir)

    epg_source_list = []
    if args.epg_url:
         epg_source_list = args.epg_url
         print("æ£€æµ‹åˆ°å‘½ä»¤è¡ŒEPGå‚æ•°ï¼Œä¼˜å…ˆä½¿ç”¨ï¼")
    elif 'epg_urls' in config and isinstance(config['epg_urls'], list):
         epg_source_list = config['epg_urls']
         print("æ­£åœ¨ä» config.json åŠ è½½EPGæºåˆ—è¡¨...")
    else:
         epg_source_list = ['https://live.fanmingming.com/e.xml']
         print("æœªæ‰¾åˆ°ä»»ä½•EPGé…ç½®ï¼Œä½¿ç”¨å†…ç½®å¤‡ç”¨åœ°å€ã€‚")
    args.epg_url = epg_source_list

    HEADERS = config.get('headers', {})
    URL_TEST_TIMEOUT = config.get('url_test_timeout', 15)
    CLOCK_URL = config.get('clock_url', "")
    
    try:
        asyncio.run(main(args))
    except KeyboardInterrupt:
        print("\næ”¶åˆ°å“¥å“¥çš„æŒ‡ä»¤ï¼Œç¨‹åºæå‰ç»“æŸã€‚")

### **ğŸ m3u8_organizer.py v20.0 [è¡€è‚‰å½’ä½Â·çœŸå¿ƒä¸æ‚”ç‰ˆ]**

# m3u8_organizer.py v20.0 - å‡¤å‡°Â·è¡€è‚‰å½’ä½Â·çœŸå¿ƒä¸æ‚”ç‰ˆ
# ä½œè€…ï¼šæ—å©‰å„¿ & å“¥å“¥
# çŠ¶æ€ï¼š100% å®Œæ•´è¿˜åŸ v14.0 æ‰€æœ‰å¤„ç†é€»è¾‘ï¼Œä»…æ¤å…¥ EPG å‡€åŒ–ã€4K åˆ†ç±»ä¸éœ“è™¹å›¾æ ‡
# è­¦å‘Šï¼šæ­¤ç‰ˆæœ¬åŒ…å«æ‰€æœ‰é˜²å¾¡æ€§ä»£ç ï¼Œç»æ— ä»»ä½•åˆ å‡ï¼

import asyncio
import aiohttp
import re
import argparse
import os
import random
import gzip
import xml.etree.ElementTree as ET
from datetime import datetime, timedelta, timezone
import shutil
import json
from urllib.parse import urlparse, urljoin
from tqdm.asyncio import tqdm_asyncio 

# --- âœ¨âœ¨âœ¨ GPSå®šä½æ¨¡å— (å®Œå…¨è¿˜åŸ) âœ¨âœ¨âœ¨ ---
BASE_DIR = os.path.dirname(os.path.abspath(__file__))

# --- âœ¨âœ¨âœ¨ å©‰å„¿çš„éœ“è™¹å›¾æ ‡é›† (é¢œå€¼ä¿éšœ) âœ¨âœ¨âœ¨ ---
GROUP_ICONS = {
    "å©‰å„¿ä¸ºå“¥å“¥æ•´ç†": "ğŸ’– å©‰å„¿Â·ç§è—ç²¾å“",
    "ğŸ’ å‡¤å‡° 4K ææ¸…": "ğŸ’ å‡¤å‡°Â·4Kææ¸…",
    "æˆ‘çš„æœ€çˆ±": "â­ æˆ‘çš„æœ€çˆ±",
    "å¤®è§†": "ğŸ“º å¤®è§†é¢‘é“",
    "å«è§†": "ğŸ“¡ å«è§†é¢‘é“",
    "æ¸¯æ¾³å°": "ğŸŒ æ¸¯æ¾³æµ·å¤–",
    "ä½“è‚²": "âš½ ä½“è‚²ç«æŠ€",
    "ç”µå½±": "ğŸ¬ ç”µå½±é¢‘é“",
    "å°‘å„¿": "ğŸ‘¶ å°‘å„¿åŠ¨ç”»",
    "çºªå½•": "ğŸ“œ çºªå½•ç‰‡",
    "ç»¼è‰º": "ğŸ¤ ç»¼è‰ºé¢‘é“",
    "æ–°é—»": "ğŸ“° æ–°é—»èµ„è®¯",
    "åœ°æ–¹": "ğŸ˜ï¸ åœ°æ–¹é¢‘é“",
    "å…¶ä»–": "ğŸŒ€ å…¶å®ƒé¢‘é“"
}

def get_pretty_group(group_name):
    """ä¸ºæ¯ç‡¥çš„åˆ†ç»„åæŠ«ä¸Šéœ“è™¹å¤–è¡£"""
    return GROUP_ICONS.get(group_name, f"ğŸ’  {group_name}")

# --- âœ¨âœ¨âœ¨ å©‰å„¿çš„ç²¾å‡†æ¸…æ´—å¼•æ“ (æ ¹æœ¬è§£å†³ EPG åŒ¹é…é—®é¢˜) âœ¨âœ¨âœ¨ ---
def get_epg_id(name):
    """ã€æ ¹æœ¬ä¿®å¤ã€‘ç”Ÿæˆæ’åº“IDï¼š'009 CCTV-1 4K' -> 'CCTV1' (ç²¾å‡†å¯¹æ¥ XML åº“)"""
    if not name: return ""
    n = name.upper().replace("CCTB", "CCTV").replace(" ", "")
    # 1. ç§»é™¤æ‰€æœ‰æ‹¬å·åŠå†…å®¹
    n = re.sub(r'[\(\[\ï¼ˆ\ã€].*?[\)\]\ï¼‰\ \ã€‘]', '', n)
    # 2. CCTVç³»åˆ—æ ‡å‡†åŒ–ï¼šCCTV-1, CCTV-13 -> CCTV1
    cctv_match = re.search(r'CCTV[-_ ]*(\d+)', n)
    if cctv_match: return f"CCTV{cctv_match.group(1)}"
    # 3. ç§»é™¤å¹²æ‰°åç¼€
    suffixes = ['é«˜æ¸…', 'æ ‡æ¸…', 'é¢‘é“', 'è¶…æ¸…', 'FHD', 'HD', 'SD', '1080P', '720P', '4K', '8K', 'UHD', 'ç›´æ’­', 'ç»¼åˆ', 'è´¢ç»', 'ç»¼è‰º', 'ä½“è‚²', 'ç”µå½±', 'å°‘å„¿', 'æ–°é—»']
    for s in suffixes: n = n.replace(s, "")
    # 4. ç§»é™¤è¡Œé¦–æ•°å­—åºå·
    n = re.sub(r'^\d+[\.\-\s]*', '', n)
    # 5. ä»…ä¿ç•™æ ¸å¿ƒå­—ç¬¦ç”¨äºæ’åº“
    n = re.sub(r'[^\w\u4e00-\u9fa5]', '', n)
    return n.strip()

def get_pretty_display_name(name):
    """ã€è§†è§‰åã€‘ä¿ç•™ 4K çµé­‚ï¼šå»æ‰åºå·ï¼Œä¿®æ­£æ‹¼å†™ï¼Œä¿ç•™ 4K æ ‡å¿—"""
    if not name: return ""
    n = re.sub(r'^\d+[\.\-\s]*', '', name) # ä»…å»åºå·
    n = n.replace('CCTB', 'CCTV').replace('[', ' ').replace(']', ' ').replace('(', ' ').replace(')', ' ')
    # è¡¥é½ 4K æ ‡å¿— (å¦‚æœåå­—é‡Œæ¼äº†)
    if any(k in name.upper() for k in ["4K", "8K", "UHD", "è¶…é«˜æ¸…"]) and "4K" not in n.upper():
        n = n + " 4K"
    return n.strip().replace("  ", " ")

def is_4k_channel(name):
    """æ¢æµ‹ 4K/ææ¸…é¢‘é“"""
    return any(k in name.upper() for k in ["4K", "8K", "UHD", "è¶…é«˜æ¸…", "ææ¸…"])

### **ã€m3u8_organizer.py v20.0 Â· ç¬¬äºŒéƒ¨åˆ†ï¼šé…ç½®ä¸­å¿ƒä¸åŸºç¡€å·¥å…· (è¿˜åŸç‰ˆ)ã€‘**

# --- é…ç½®åŠ è½½åŒº (100% å®Œæ•´è¿˜åŸ v14.0 é€»è¾‘) ---
def load_global_config(config_path):
    abs_path = os.path.join(BASE_DIR, config_path)
    default_config = {
        "headers": { 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36' },
        "url_test_timeout": 15, # âœ¨ è¿˜åŸå“¥å“¥æœ€æ”¾å¿ƒçš„15ç§’è¶…æ—¶
        "clock_url": "http://epg.pw/zdy/clock.m3u8"
    }
    try:
        if os.path.exists(abs_path):
            with open(abs_path, 'r', encoding='utf-8') as f:
                print(f"æ­£åœ¨ä» {abs_path} åŠ è½½å¤–éƒ¨é…ç½®...")
                user_config = json.load(f)
                # âœ¨âœ¨âœ¨ è¿˜åŸæå…¶ä¸¥è°¨çš„é€’å½’æ›´æ–°é€»è¾‘ âœ¨âœ¨âœ¨
                for key, value in user_config.items():
                    if isinstance(value, dict) and key in default_config and isinstance(default_config[key], dict):
                        default_config[key].update(value)
                    else:
                        default_config[key] = value
                print("å¤–éƒ¨é…ç½®åŠ è½½æˆåŠŸï¼")
        else:
            print(f"é…ç½®æ–‡ä»¶ {abs_path} æœªæ‰¾åˆ°ï¼Œå°†ä½¿ç”¨é»˜è®¤é…ç½®ã€‚")
    except Exception as e:
        print(f"åŠ è½½å…¨å±€é…ç½®æ–‡ä»¶ {abs_path} å¤±è´¥: {e}ï¼Œå°†ä½¿ç”¨é»˜è®¤é…ç½®ã€‚")
    return default_config

def load_category_rules_from_dir(rules_dir):
    abs_path = os.path.join(BASE_DIR, rules_dir)
    category_rules = {}
    if not os.path.isdir(abs_path):
        print(f"ã€è­¦å‘Šã€‘è§„åˆ™ç›®å½• '{abs_path}' ä¸å­˜åœ¨ï¼")
        return {}
    print(f"æ­£åœ¨ä»ã€è§„åˆ™åº“ã€‘'{abs_path}' åŠ è½½åˆ†ç±»è§„åˆ™...")
    for filename in os.listdir(abs_path):
        if filename.endswith('.txt'):
            category_name = os.path.splitext(filename)[0]
            filepath = os.path.join(abs_path, filename)
            keywords = load_list_from_file(filepath)
            if keywords:
                category_rules[category_name] = keywords
    return category_rules

# --- å…¨å±€å˜é‡å£°æ˜ ---
HEADERS = {}
URL_TEST_TIMEOUT = 15
CATEGORY_RULES = {}
CLOCK_URL = ""

# --- å·¥å…·å‡½æ•°åŒº (å®Œå…¨å¯¹é½ v14.0) ---
def load_list_from_file(filename):
    abs_path = os.path.join(BASE_DIR, filename)
    if not filename or not os.path.exists(abs_path):
        if filename: print(f"  - é…ç½®æ–‡ä»¶ {abs_path} æœªæ‰¾åˆ°ï¼Œå°†è·³è¿‡ã€‚")
        return []
    try:
        with open(abs_path, 'r', encoding='utf-8') as f:
            return [line.strip() for line in f if line.strip() and not line.startswith('#')]
    except Exception as e:
        print(f"  - è¯»å–é…ç½®æ–‡ä»¶ {abs_path} å¤±è´¥: {e}")
        return []

### **ã€m3u8_organizer.py v20.0 Â· ç¬¬ä¸‰éƒ¨åˆ†ï¼šæ‰‹åŠ¨é‡å®šå‘è´¨æ£€å‘˜ä¸è§£æå¼•æ“ã€‘**

# --- âœ¨âœ¨âœ¨ ã€è¿˜åŸã€‘ç»ˆæè¿½è¸ªç‰ˆè´¨æ£€å‘˜ (å®Œå…¨è¿˜åŸ v14.0 æ‰‹åŠ¨é‡å®šå‘é€»è¾‘) âœ¨âœ¨âœ¨ ---
async def test_url(session, url):
    """æµ‹è¯•å•ä¸ªURLçš„å»¶è¿Ÿï¼Œå¹¶æ‰‹åŠ¨å¤„ç†é‡å®šå‘ï¼Œç¡®ä¿è¿½åˆ°çœŸå®ä¿¡å·"""
    try:
        start_time = asyncio.get_event_loop().time()
        # âœ¨ å®Œå…¨è¿˜åŸå“¥å“¥çš„ allow_redirects=False æ‰‹åŠ¨å¤„ç†é€»è¾‘
        async with session.get(url, headers=HEADERS, timeout=URL_TEST_TIMEOUT, allow_redirects=False) as response:
            # å¦‚æœæ˜¯é‡å®šå‘ (301, 302, 307, 308)
            if response.status in [301, 302, 307, 308]:
                redirected_url = response.headers.get('Location')
                # å¤„ç†ç›¸å¯¹è·¯å¾„é‡å®šå‘
                if redirected_url and not redirected_url.startswith('http'):
                    base_url = urlparse.urljoin(url, '.')
                    redirected_url = urlparse.urljoin(base_url, redirected_url)

                if redirected_url:
                    # è¿½éšæ–°åœ°å€ï¼Œå¹¶å¸¦ä¸Š Referer
                    new_headers = HEADERS.copy()
                    new_headers['Referer'] = url 
                    # ç»™ç¬¬äºŒæ¬¡è¯·æ±‚ä¸€ä¸ªç¨çŸ­çš„è¶…æ—¶
                    async with session.get(redirected_url, headers=new_headers, timeout=URL_TEST_TIMEOUT - 3, allow_redirects=False) as redirected_response:
                        if 200 <= redirected_response.status < 300:
                            end_time = asyncio.get_event_loop().time()
                            return url, (end_time - start_time) * 1000
            # å¦‚æœæ˜¯ç›´æ¥æˆåŠŸ...
            elif 200 <= response.status < 300:
                end_time = asyncio.get_event_loop().time()
                return url, (end_time - start_time) * 1000

            return url, float('inf')
    except (aiohttp.ClientError, asyncio.TimeoutError):
        return url, float('inf')
    except Exception:
        return url, float('inf')

# --- ä¿¡å·è§£æå¼•æ“ (100% è¿˜åŸ v14.0 â€œæ™ºèƒ½åˆ†æµç‰ˆâ€è§£æå™¨) ---
def parse_m3u_content(content, ad_keywords):
    """ä¸“é—¨è§£æ M3U æ ¼å¼ï¼Œå¸¦ tvg-name æå–ä¸å¹¿å‘Šè¿‡æ»¤"""
    channels = {}
    processed_urls = set()
    def add_channel(name, url):
        name = name.strip().replace(" ", "") # è¿˜åŸå“¥å“¥çš„ç©ºæ ¼æ¸…ç†
        url = url.strip()
        if not name or not url or url in processed_urls: return
        if any(keyword in name for keyword in ad_keywords): return
        if name not in channels: channels[name] = []
        channels[name].append(url)
        processed_urls.add(url)

    lines = content.split('\n')
    for i, line in enumerate(lines):
        line = line.strip()
        if not line or not line.startswith('#EXTINF:'): continue
        try:
            if i + 1 < len(lines) and not lines[i+1].strip().startswith('#'):
                url = lines[i+1].strip()
                # ä¼˜å…ˆå¯»æ‰¾ tvg-nameï¼Œæ²¡æœ‰åˆ™å–æœ€åçš„åå­—
                name_match = re.search(r'tvg-name="([^"]*)"', line)
                name = name_match.group(1) if name_match else line.split(',')[-1]
                add_channel(name, url)
        except Exception:
            continue
    return channels

def parse_txt_content(content, ad_keywords):
    """ä¸“é—¨è§£æ TXT æ ¼å¼ï¼Œå¸¦å¹¿å‘Šè¿‡æ»¤ä¸å¥å£®æ€§æ£€æŸ¥"""
    channels = {}
    processed_urls = set()
    def add_channel(name, url):
        name = name.strip().replace(" ", "")
        url = url.strip()
        if not name or not url or url in processed_urls: return
        if any(keyword in name for keyword in ad_keywords): return
        if name not in channels: channels[name] = []
        channels[name].append(url)
        processed_urls.add(url)

    for line in content.split('\n'):
        line = line.strip()
        if not line or line.startswith('#') or '#genre#' in line: continue
        if ',' in line and 'http' in line:
            try:
                last_comma_index = line.rfind(',')
                name = line[:last_comma_index]
                url = line[last_comma_index+1:]
                if url.startswith('http'): add_channel(name, url)
            except Exception:
                continue
    return channels

# --- âœ¨âœ¨âœ¨ EPG æ•°æ®ä¸­å¿ƒ (åŒå‘å‡€åŒ–å¯¹é½) âœ¨âœ¨âœ¨ ---
async def load_epg_data(epg_url):
    """è¿˜åŸ GZIP å¤„ç†é€»è¾‘ï¼Œå¹¶æ¤å…¥ get_epg_id å®ç° ID æ ¹æœ¬åŒ¹é…"""
    if not epg_url: return {}
    print(f"\nğŸ“¡ æ­£åœ¨åŠ è½½ EPG æ•°æ®: {epg_url}...")
    epg_data = {}
    try:
        content_bytes = b''
        async with aiohttp.ClientSession() as session:
            async with session.get(epg_url, headers=HEADERS, timeout=30) as response:
                content_bytes = await response.read()

        # å¤„ç† GZIP å‹ç¼© (å®Œå…¨è¿˜åŸ)
        if content_bytes.startswith(b'\x1f\x8b'):
            content = gzip.decompress(content_bytes).decode('utf-8')
        else:
            content = content_bytes.decode('utf-8')

        root = ET.fromstring(content)
        for channel in root.findall('channel'):
            display_name_tag = channel.find('display-name')
            if display_name_tag is not None and display_name_tag.text:
                raw_name = display_name_tag.text.strip()
                # ã€æ–°åŠŸèƒ½æ³¨å…¥ã€‘ç”¨ get_epg_id æ¸…æ´—ï¼Œç¡®ä¿å¯¹é½ CCTV1 æ ¼å¼
                cleaned_epg_id = get_epg_id(raw_name)
                channel_id = channel.get('id', raw_name)
                icon_tag = channel.find('icon')
                logo_url = icon_tag.get('src', "") if icon_tag is not None else ""
                epg_data[cleaned_epg_id] = {"tvg-id": channel_id, "tvg-logo": logo_url}
        print(f"  - âœ… EPGåŠ è½½æˆåŠŸï¼å…±è§£æå‡º {len(epg_data)} ä¸ªç‰¹å¾ã€‚")
    except Exception as e:
        print(f"  - âŒ EPGæ•°æ®åŠ è½½å¤±è´¥: {e}")
    return epg_data

def classify_channel(channel_name):
    """è¿˜åŸè§„åˆ™åˆ†ç±»é€»è¾‘"""
    for category, keywords in CATEGORY_RULES.items():
        if any(keyword in channel_name for keyword in keywords):
            return category
    return "å…¶ä»–"

### **ã€m3u8_organizer.py v20.0 Â· ç¬¬å››éƒ¨åˆ†ï¼šEPG è½®è¯¢ä¸ä¸‡æºå½’å®—ã€‘**

async def main(args):
    """ä¸»æ‰§è¡Œå‡½æ•°ï¼šå‡¤å‡°ç³»ç»Ÿçš„å®Œå…¨ä½“å¼•æ“"""
    print(f"æŠ¥å‘Šå“¥å“¥ï¼Œå©‰å„¿çš„â€œè¶…çº§èŠ‚ç›®å•â€ v20.0ã€è¡€è‚‰å½’ä½ç‰ˆã€‘å¼€å§‹å·¥ä½œå•¦ï¼")

    # --- âœ¨ EPG å¤„ç†é€»è¾‘ (1:1 è¿˜åŸ v14.0ï¼Œç»æ— ç¼©å‡) ---
    epg_backup_list = args.epg_url[:3]
    top_3_epgs_str = ",".join(epg_backup_list)
    print(f"\nEPGå¤„ç†ï¼šæœ€ç»ˆå°†å†™å…¥è¿™å‡ ä¸ªEPGæºåˆ°æ–‡ä»¶: {top_3_epgs_str}")

    epg_data = {}
    for epg_url in epg_backup_list:
        temp_epg_data = await load_epg_data(epg_url)
        if temp_epg_data:
            epg_data = temp_epg_data
            print(f"  - âœ… æœ¬æ¬¡è¿è¡Œé€‰ç”¨EPGä¸»æº: {epg_url}")
            break
    if not epg_data:
        print("  - âš ï¸ è­¦å‘Šï¼šæ‰€æœ‰EPGæºå‡ä¸å¯ç”¨ï¼")

    ad_keywords = load_list_from_file(args.blacklist)
    favorite_channels = load_list_from_file(args.favorites)

    # --- ç¬¬ä¸€æ­¥ï¼šã€ä¸‡æºå½’å®—ã€‘(100% è¿˜åŸ v14.0 æŠ“å–ç»†èŠ‚) ---
    print("\nç¬¬ä¸€æ­¥ï¼šã€ä¸‡æºå½’å®—ã€‘æ­£åœ¨èåˆæ‰€æœ‰ä¿¡å·æº...")
    all_channels_pool = {}

    # 1. æŠ“å–æœ¬åœ°ã€ç§å­ä»“åº“ã€‘
    manual_sources_abs_dir = os.path.join(BASE_DIR, args.manual_sources_dir)
    if os.path.isdir(manual_sources_abs_dir):
        print(f"  - ğŸ“‚ è¯»å–ã€ç§å­ä»“åº“ã€‘: {manual_sources_abs_dir}")
        for filename in os.listdir(manual_sources_abs_dir):
            filepath = os.path.join(manual_sources_abs_dir, filename)
            if os.path.isfile(filepath):
                with open(filepath, 'r', encoding='utf-8') as f:
                    content = f.read()
                    # æ ¹æ®åç¼€é€‰æ‹©è§£æå™¨
                    if filename.endswith('.m3u'):
                        channels = parse_m3u_content(content, ad_keywords)
                    else:
                        channels = parse_txt_content(content, ad_keywords)
                    
                    for name, urls in channels.items():
                        if name not in all_channels_pool:
                            all_channels_pool[name] = {"urls": set(), "source_type": "manual"}
                        all_channels_pool[name]["urls"].update(urls)

    # 2. æŠ“å–ã€ç½‘ç»œäº‘ç«¯æºã€‘(1:1 è¿˜åŸ fetch_and_parse å¼‚æ­¥å¾ªç¯)
    remote_sources_abs_file = os.path.join(BASE_DIR, args.remote_sources_file)
    if os.path.exists(remote_sources_abs_file):
        print(f"  - ğŸŒ æ­£åœ¨åŒæ­¥ç½‘ç»œäº‘ç«¯ä¿¡å·...")
        remote_urls = load_list_from_file(args.remote_sources_file)
        
        # ç–¾é£ä¼˜åŒ–ï¼šå¼€å¯ DNS ç¼“å­˜ä¸è¿æ¥æ± 
        connector = aiohttp.TCPConnector(limit=0, ttl_dns_cache=300)
        async with aiohttp.ClientSession(connector=connector) as session:
            tasks = []
            for url in remote_urls:
                async def fetch_and_parse(remote_url):
                    try:
                        async with session.get(remote_url, headers=HEADERS, timeout=20) as response:
                            content = await response.text(encoding='utf-8', errors='ignore')
                            if remote_url.endswith('.m3u'):
                                channels = parse_m3u_content(content, ad_keywords)
                            else:
                                channels = parse_txt_content(content, ad_keywords)
                            for name, urls in channels.items():
                                if name not in all_channels_pool:
                                    all_channels_pool[name] = {"urls": set(), "source_type": "network"}
                                all_channels_pool[name]["urls"].update(urls)
                    except Exception:
                        pass
                tasks.append(fetch_and_parse(url))
            await asyncio.gather(*tasks)

    unique_urls_count = sum(len(data["urls"]) for data in all_channels_pool.values())
    print(f"  - âœ… èåˆå®Œæˆï¼å…±æ”¶é›†åˆ° {len(all_channels_pool)} ä¸ªé¢‘é“ï¼Œ{unique_urls_count} æ¡ç‹¬ç«‹çº¿è·¯ã€‚")

### **ã€m3u8_organizer.py v20.0 Â· ç¬¬äº”éƒ¨åˆ†ï¼šåƒäººè¯•ç‚¼ä¸ç›²ç›’çµé­‚å›å½’ã€‘**

    # --- ç¬¬äºŒæ­¥ï¼šã€ç»ˆæè¯•ç‚¼ã€‘(1000å¹¶å‘ + ç›²ç›’åŒæ­¥æ‰«æ) ---
    print("\nç¬¬äºŒæ­¥ï¼šã€ç»ˆæè¯•ç‚¼ã€‘æ­£åœ¨æ£€éªŒæ‰€æœ‰åœ°å€çš„å¯ç”¨æ€§...")
    all_urls_to_test = {url for data in all_channels_pool.values() for url in data["urls"]}
    
    # âœ¨âœ¨âœ¨ ã€å®Œå…¨è¿˜åŸã€‘æ ¸å¿ƒæ‰¾å›ï¼šç›²ç›’(Picks)æºä¸€èµ·å‚åŠ â€œå¤§æ¯”æ­¦â€ âœ¨âœ¨âœ¨
    picks_abs_dir = os.path.join(BASE_DIR, args.picks_dir)
    if os.path.isdir(picks_abs_dir):
        for pick_file in os.listdir(picks_abs_dir):
            pick_path = os.path.join(picks_abs_dir, pick_file)
            if os.path.isfile(pick_path) and pick_file.endswith('.txt'):
                with open(pick_path, 'r', encoding='utf-8') as pf:
                    for line in pf:
                        line = line.strip()
                        if not line or line.startswith('#'): continue
                        try:
                            # 1:1 è¿˜åŸå“¥å“¥ v14.0 çš„åˆ†å‰²é€»è¾‘
                            url = line.split(',')[-1]
                            if url.startswith('http'): all_urls_to_test.add(url)
                        except IndexError:
                            if line.startswith('http'): all_urls_to_test.add(line)

    url_speeds = {}
    # âœ¨ ç–¾é£é…ç½®ï¼šè§£é™¤è¿æ¥æ± é™åˆ¶
    semaphore = asyncio.Semaphore(1000) 

    async def limited_test_url(session, url):
        async with semaphore:
            return await test_url(session, url)

    # æ ¸å¿ƒï¼šä½¿ç”¨å¸¦åŠ é€Ÿçš„ TCPConnector
    connector = aiohttp.TCPConnector(limit=0, ttl_dns_cache=300)
    async with aiohttp.ClientSession(connector=connector) as session:
        tasks = [limited_test_url(session, url) for url in all_urls_to_test]
        results = []
        # ä½¿ç”¨ tqdm å±•ç°ç–¾é£èˆ¬çš„é€Ÿåº¦
        for f in tqdm_asyncio.as_completed(tasks, total=len(tasks), desc="âš¡ å‡¤å‡°è´¨æ£€"):
            results.append(await f)
        for url, speed in results:
            url_speeds[url] = speed

    valid_url_count = sum(1 for speed in url_speeds.values() if speed != float('inf'))
    print(f"\n  - è¯•ç‚¼å®Œæˆï¼å­˜æ´»èŠ‚ç‚¹ {valid_url_count}/{len(all_urls_to_test)}ã€‚")

    # --- ç¬¬ä¸‰æ­¥ï¼šã€ç”Ÿæ€è¿›åŒ–ã€‘(1:1 è¿˜åŸåˆ†ç±»ç»†èŠ‚ + æ¤å…¥4Kæ‹¦æˆª) ---
    print("\nç¬¬ä¸‰æ­¥ï¼šã€ç”Ÿæ€è¿›åŒ–ã€‘æ­£åœ¨ä¸ºå¹¸å­˜è€…å½’ç±»å¹¶ç­›é€‰ 4K ä¿¡å·...")
    survivors_classified = {}
    GROUP_4K = "ğŸ’ å‡¤å‡° 4K ææ¸…"

    for name, data in all_channels_pool.items():
        # è·å–æœ‰æ•ˆçº¿è·¯å¹¶æŒ‰é€Ÿåº¦æ’åº
        valid_urls = [url for url in data["urls"] if url_speeds.get(url, float('inf')) != float('inf')]
        if valid_urls:
            valid_urls.sort(key=lambda u: url_speeds[u])
            
            # âœ¨ æ–°å¢é€»è¾‘ï¼š4K æ™ºèƒ½æ‹¦æˆª
            if is_4k_channel(name):
                category = GROUP_4K
            else:
                category = classify_channel(name)
            
            if category not in survivors_classified:
                survivors_classified[category] = {}
            if name not in survivors_classified[category]:
                 survivors_classified[category][name] = []

            # çº¿è·¯ä¿ç•™ç­–ç•¥ (1:1 è¿˜åŸ v14.0 æ¯ä¸€ä¸ª if)
            if data["source_type"] == "manual":
                survivors_classified[category][name].extend(valid_urls)
            else:
                # ç½‘ç»œæºå–æœ€å¿«å‰ 5
                survivors_classified[category][name].extend(valid_urls[:5])

    print(f"  - âœ… ç”Ÿæ€è¿›åŒ–å®Œæˆï¼å¹¸å­˜é¢‘é“å·²æŒ‰éƒ¨å°±ç­å½’é˜Ÿã€‚")

    # --- ç¬¬å››æ­¥ï¼šã€èåˆè¾“å‡ºã€‘(å®Œå…¨è¿˜åŸåŒæ ¼å¼è¾“å‡ºé€»è¾‘) ---
    print("\nç¬¬å››æ­¥ï¼šã€èåˆè¾“å‡ºã€‘æ­£åœ¨å‡†å¤‡ç”Ÿæˆæœ€ç»ˆèŠ‚ç›®å•...")
    output_abs_path = os.path.join(BASE_DIR, args.output)
    m3u_filename = f"{output_abs_path}.m3u"
    txt_filename = f"{output_abs_path}.txt"
    os.makedirs(os.path.dirname(m3u_filename), exist_ok=True)

    beijing_time = datetime.now(timezone(timedelta(hours=8))).strftime('%Y-%m-%d %H:%M:%S')

    # âœ¨âœ¨âœ¨ ã€å®Œå…¨è¿˜åŸã€‘çœŸÂ·ç›²ç›’éšæœºé€»è¾‘ (v14.0 æ¯ä¸€ä¸ª print éƒ½è¿˜åœ¨ï¼) âœ¨âœ¨âœ¨
    blind_box_group_name = "å©‰å„¿ä¸ºå“¥å“¥æ•´ç†"
    blind_box_channels = {}
    if os.path.isdir(picks_abs_dir):
        print("  - å‘ç°ã€æ¯æ—¥ç²¾é€‰ã€‘ç›²ç›’ï¼Œæ­£åœ¨å¼€å¯å¹¸è¿æº...")
        pick_files = sorted(os.listdir(picks_abs_dir))
        for pick_file in pick_files:
            pick_path = os.path.join(picks_abs_dir, pick_file)
            if os.path.isfile(pick_path) and pick_file.endswith('.txt'):
                pick_name = os.path.splitext(pick_file)[0]
                with open(pick_path, 'r', encoding='utf-8') as pf:
                    pick_content = pf.read()
                    # è¿˜åŸå“¥å“¥ v14.0 çš„ç›²ç›’å†…éƒ¨è§£æå’ŒéšæœºæŠ½å–é€»è¾‘
                    pick_channels_data = parse_txt_content(pick_content, ad_keywords)
                    valid_urls_in_file = [url for urls in pick_channels_data.values() for url in urls if url_speeds.get(url, float('inf')) != float('inf')]

                    if valid_urls_in_file:
                        random_url = random.choice(valid_urls_in_file)
                        safe_pick_name = pick_name.replace(" ", "-")
                        blind_box_channels[safe_pick_name] = [random_url]
                        print(f"    - ç›²ç›’ '{pick_name}' å·²å¼€å¯ï¼Œå¹¸è¿æºï¼š{random_url[:30]}...")
                    else:
                        print(f"    - ç›²ç›’ '{pick_name}' å·²å¤±æ•ˆã€‚")

### **ã€m3u8_organizer.py v20.0 Â· ç¬¬å…­éƒ¨åˆ†ï¼šå…¨é‡æ’åºã€åŒæ ¼å¼è¾“å‡ºä¸å…¥å£å¤§ç®¡å®¶ (å®Œç»“)ã€‘**

    # 2. å‡†å¤‡å¸¸è§„åˆ†ç»„å¹¶å¤„ç†æ”¶è—
    final_grouped_channels = {}
    if blind_box_channels:
        final_grouped_channels[blind_box_group_name] = blind_box_channels

    for category, channels in survivors_classified.items():
        for name, urls in channels.items():
            # è¿˜åŸ v14.0 æ”¶è—å¤¹åˆ¤å®šé€»è¾‘
            group_name = "æˆ‘çš„æœ€çˆ±" if name in favorite_channels else category
            if group_name not in final_grouped_channels:
                final_grouped_channels[group_name] = {}
            if name not in final_grouped_channels[group_name]:
                 final_grouped_channels[group_name][name] = []
            final_grouped_channels[group_name][name].extend(urls)

    # 3. âœ¨âœ¨âœ¨ ã€å®Œå…¨è¿˜åŸã€‘ç¡®å®šæœ€ç»ˆçš„é»„é‡‘æ’åºé€»è¾‘ âœ¨âœ¨âœ¨
    prefix_order = ["å©‰å„¿ä¸ºå“¥å“¥æ•´ç†", GROUP_4K, "æˆ‘çš„æœ€çˆ±", "å¤®è§†", "å«è§†", "æ¸¯æ¾³å°"]
    all_existing_groups = list(final_grouped_channels.keys())
    ordered_groups = []

    for group in prefix_order:
        if group in all_existing_groups:
            ordered_groups.append(group)
            all_existing_groups.remove(group)

    other_group_exists = "å…¶ä»–" in all_existing_groups
    if other_group_exists:
        all_existing_groups.remove("å…¶ä»–")

    ordered_groups.extend(sorted(all_existing_groups))

    if other_group_exists:
        ordered_groups.append("å…¶ä»–")

    # 4. âœ¨âœ¨âœ¨ ã€å®Œå…¨è¿˜åŸã€‘é»„é‡‘å¤§å¾ªç¯ï¼šæŒ‰ç…§é¡ºåºåŒæ­¥å†™å…¥ M3U ä¸ TXT âœ¨âœ¨âœ¨
    with open(m3u_filename, 'w', encoding='utf-8') as f_m3u, open(txt_filename, 'w', encoding='utf-8') as f_txt:
        # å†™å…¥åœ°è¡¨æœ€å¼ºå¤´éƒ¨å®šä¹‰ (æ”¯æŒå¤š EPG è½®è¯¢)
        f_m3u.write(f'#EXTM3U x-tvg-url="{top_3_epgs_str}" tvg-url="{top_3_epgs_str}" catchup="append" catchup-source="?playseek=${{(b)yyyyMMddHHmmss}}-${{(e)yyyyMMddHHmmss}}"\n')
        
        # å†™å…¥æ›´æ–°æ—¶é—´
        f_m3u.write(f'#EXTINF:-1 group-title="ğŸ•’ å‡¤å‡°Â·æ›´æ–°æ—¶é—´",å‡¤å‡°æ›´æ–°æ—¶é—´({beijing_time})\n{CLOCK_URL}\n')
        f_txt.write(f'æ›´æ–°æ—¶é—´,#genre#\n{beijing_time},{CLOCK_URL}\n\n')

        for group in ordered_groups:
            # âœ¨ é¢œå€¼å‡çº§ï¼šå¸¦ä¸Šå©‰å„¿çš„éœ“è™¹å›¾æ ‡
            pretty_group_name = get_pretty_group(group)
            f_txt.write(f'{pretty_group_name},#genre#\n')

            channels_in_group = final_grouped_channels.get(group)
            if not channels_in_group: continue

            for name, urls in sorted(channels_in_group.items()):
                # ã€æ ¸å¿ƒè¿›åŒ–ã€‘ä¸¤å¥—åå­—ï¼Œä¸€ä¸ªæ’åº“(é€‚é…CCTV1æ ¼å¼)ï¼Œä¸€ä¸ªè§†è§‰æ˜¾ç¤º(å¸¦4K)
                eid = get_epg_id(name)               # ç”¨äºæ‰¾èŠ‚ç›®å• (CCTV1)
                disp = get_pretty_display_name(name) # ç”¨äºå±å¹•æ˜¾ç¤º (CCTV-1 4K)
                
                # åŒå‘å¯¹é½ï¼šåœ¨ EPG åº“ä¸­å¯»æ‰¾åŒ¹é…
                info = epg_data.get(eid, {})
                tid = info.get("tvg-id", eid)
                logo = info.get("tvg-logo", "")

                for url in urls:
                    # A. å†™å…¥ TXT æ ¼å¼ (è¿˜åŸç»†èŠ‚)
                    f_txt.write(f'{disp},{url}\n')
                    
                    # B. âœ¨âœ¨âœ¨ ã€å®Œå…¨è¿˜åŸã€‘ä¸‰å¥— Catchup åè®®ç²¾å‡†é€‚é… (v14.0 ç²¾é«“) âœ¨âœ¨âœ¨
                    catchup_tag = ""
                    if any(x in url for x in ["PLTV", "TVOD", "/liveplay/", "/replay/"]):
                        catchup_tag = ' catchup="append" catchup-source="?playseek=${(b)yyyyMMddHHmmss}-${(e)yyyyMMddHHmmss}"'
                    elif ".m3u8" in url and ("playback" in url or "replay" in url):
                         catchup_tag = ' catchup="append" catchup-source="?starttime=${(b)yyyyMMddHHmmss}&endtime=${(e)yyyyMMddHHmmss}"'
                    elif ".php" in url and "id=" in url:
                         catchup_tag = ' catchup="append" catchup-source="&playseek=${(b)yyyyMMddHHmmss}-${(e)yyyyMMddHHmmss}"'

                    # C. æœ€ç»ˆå†™å…¥ M3U æ ¸å¿ƒè¡Œï¼štvg-id å’Œ tvg-name å…¨éƒ¨å¯¹é½æç®€ ID (è§£å†³èœå•æ¶ˆå¤±)
                    f_m3u.write(f'#EXTINF:-1 tvg-id="{tid}" tvg-name="{tid}" tvg-logo="{logo}" group-title="{pretty_group_name}"{catchup_tag},{disp}\n')
                    f_m3u.write(f'{url}\n')

            f_txt.write('\n')

    print(f"\nç¬¬äº”æ­¥ï¼šä»»åŠ¡å®Œæˆï¼æˆ‘ä»¬çš„ç”Ÿæ€ç³»ç»Ÿå·²æŒ‰é»„é‡‘é¡ºåºå®Œæˆæœ€ç»ˆè¿›åŒ–ï¼")
    print(f"  - æœ€ç»ˆæˆå“å·²ç”Ÿæˆ: {m3u_filename} (M3U) & {txt_filename} (TXT)")
    print(f"  - å©‰å„¿æŠ¥å‘Šï¼š4K å½’ä½ã€EPG æ ¹æœ¬å¯¹é½ã€ç›²ç›’çµé­‚å·²å¤äº§ï¼")

# --- âœ¨âœ¨âœ¨ ã€å®Œç’§å½’èµµã€‘å…¥å£å¤§ç®¡å®¶ (100% è¿˜åŸ v14.0 æ¯ä¸€ä¸ªå‚æ•°è¯´æ˜) âœ¨âœ¨âœ¨ ---
if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='å©‰å„¿çš„â€œè¶…çº§èŠ‚ç›®å•â€ v20.0ã€è¡€è‚‰å½’ä½Â·æœ€ç»ˆå…¨é‡ç‰ˆã€‘')

    parser.add_argument('--config', type=str, default='config.json', help='å…¨å±€JSONé…ç½®æ–‡ä»¶çš„è·¯å¾„')
    parser.add_argument('--rules-dir', type=str, default='rules', help='ã€å¤‡ç”¨ã€‘åˆ†ç±»è§„åˆ™ç›®å½•')
    parser.add_argument('--manual-sources-dir', type=str, default='sources_manual', help='ã€ç§å­ä»“åº“ã€‘æ‰‹åŠ¨ç»´æŠ¤çš„æºç›®å½•')
    parser.add_argument('--generated-sources-dir', type=str, default='sources_generated', help='ã€æˆå“ä»“åº“ã€‘è„šæœ¬è‡ªåŠ¨ç”Ÿæˆçš„æºç›®å½•')
    parser.add_argument('--remote-sources-file', type=str, default='sources.txt', help='åŒ…å«è¿œç¨‹ç›´æ’­æºURLåˆ—è¡¨çš„æ–‡ä»¶')
    parser.add_argument('--picks-dir', type=str, default='picks', help='ã€æ¯æ—¥ç²¾é€‰ã€‘ç›²ç›’æºç›®å½•')

    parser.add_argument('--epg-url', nargs='+', default=None, help='ã€è¦†ç›–ã€‘EPGæ•°æ®æºURLï¼Œä¼šè¦†ç›–é…ç½®æ–‡ä»¶ä¸­çš„è®¾ç½®')

    parser.add_argument('-b', '--blacklist', type=str, default='config/blacklist.txt', help='é¢‘é“é»‘åå•æ–‡ä»¶')
    parser.add_argument('-f', '--favorites', type=str, default='config/favorites.txt', help='æ”¶è—é¢‘é“åˆ—è¡¨æ–‡ä»¶')
    parser.add_argument('-o', '--output', type=str, default='dist/live', help='è¾“å‡ºæ–‡ä»¶çš„å‰ç¼€ï¼ˆä¸å«æ‰©å±•åï¼‰')

    args = parser.parse_args()

    # åŠ è½½é…ç½®
    config = load_global_config(args.config)

    # é€»è¾‘ï¼šåˆ†ç±»è§„åˆ™åŠ è½½
    if 'category_rules' in config and isinstance(config['category_rules'], dict):
        print("æ­£åœ¨ä» config.json åŠ è½½åˆ†ç±»è§„åˆ™...")
        CATEGORY_RULES = config['category_rules']
    else:
        print("config.json ä¸­æœªæ‰¾åˆ°åˆ†ç±»è§„åˆ™ï¼Œå°†ä» 'rules' ç›®å½•åŠ è½½ã€‚")
        CATEGORY_RULES = load_category_rules_from_dir(args.rules_dir)

    # é€»è¾‘ï¼šä¸‰çº§ EPG è½®è¯¢åˆ¤å®š
    epg_source_list = []
    if args.epg_url:
         epg_source_list = args.epg_url
         print("æ£€æµ‹åˆ°å‘½ä»¤è¡ŒEPGå‚æ•°ï¼Œä¼˜å…ˆä½¿ç”¨ï¼")
    elif 'epg_urls' in config and isinstance(config['epg_urls'], list):
         epg_source_list = config['epg_urls']
         print("æ­£åœ¨ä» config.json åŠ è½½EPGæºåˆ—è¡¨...")
    else:
         epg_source_list = ['https://live.fanmingming.com/e.xml']
         print("æœªæ‰¾åˆ°ä»»ä½•EPGé…ç½®ï¼Œä½¿ç”¨å†…ç½®å¤‡ç”¨åœ°å€ã€‚")
    args.epg_url = epg_source_list

    # åŠ è½½å…¨å±€å˜é‡
    HEADERS = config.get('headers', {})
    URL_TEST_TIMEOUT = config.get('url_test_timeout', 15)
    CLOCK_URL = config.get('clock_url', "")

    # å¯åŠ¨å¼‚æ­¥å¼•æ“
    try:
        asyncio.run(main(args))
    except KeyboardInterrupt:
        print("\næ”¶åˆ°é€€å‡ºæŒ‡ä»¤ï¼Œå©‰å„¿æ’¤é€€å•¦ï¼ğŸ‘‹")
    except Exception as e:
        print(f"\nå“å‘€ï¼Œå©‰å„¿å¥½åƒè¢«ä»£ç ç»Šå€’äº†: {e}")

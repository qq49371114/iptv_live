# m3u8_organizer.py v9.1 - æœ€ç»ˆæˆå“ç‰ˆ
# ä½œè€…ï¼šæ—å©‰å„¿ & å“¥å“¥

import asyncio
import aiohttp
import re
import argparse
import os
import random
import gzip
import xml.etree.ElementTree as ET
from datetime import datetime, timedelta, timezone
import shutil
import json

# --- âœ¨âœ¨âœ¨ GPSå®šä½æ¨¡å— âœ¨âœ¨âœ¨ ---
# è·å–è„šæœ¬æ–‡ä»¶æ‰€åœ¨ç›®å½•çš„ç»å¯¹è·¯å¾„ï¼Œä½œä¸ºæ‰€æœ‰ç›¸å¯¹è·¯å¾„çš„åŸºå‡†
BASE_DIR = os.path.dirname(os.path.abspath(__file__))

# --- é…ç½®åŠ è½½åŒº ---

def load_global_config(config_path):
    """ä»JSONæ–‡ä»¶åŠ è½½å…¨å±€é…ç½®"""
    # å°†è·¯å¾„è½¬æ¢ä¸ºç»å¯¹è·¯å¾„
    abs_path = os.path.join(BASE_DIR, config_path)
    default_config = {
        "headers": {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36'
        },
        "url_test_timeout": 8,
        "clock_url": "http://epg.pw/zdy/clock.m3u8"
    }
    try:
        if os.path.exists(abs_path):
            with open(abs_path, 'r', encoding='utf-8') as f:
                print(f"æ­£åœ¨ä» {abs_path} åŠ è½½å¤–éƒ¨é…ç½®...")
                user_config = json.load(f)
                # ä½¿ç”¨æ·±åº¦æ›´æ–°
                for key, value in user_config.items():
                    if isinstance(value, dict) and key in default_config and isinstance(default_config[key], dict):
                        default_config[key].update(value)
                    else:
                        default_config[key] = value
                print("å¤–éƒ¨é…ç½®åŠ è½½æˆåŠŸï¼")
        else:
            print(f"é…ç½®æ–‡ä»¶ {abs_path} æœªæ‰¾åˆ°ï¼Œå°†ä½¿ç”¨é»˜è®¤é…ç½®ã€‚")
    except Exception as e:
        print(f"åŠ è½½å…¨å±€é…ç½®æ–‡ä»¶ {abs_path} å¤±è´¥: {e}ï¼Œå°†ä½¿ç”¨é»˜è®¤é…ç½®ã€‚")
    return default_config

def load_category_rules_from_dir(rules_dir):
    """ä»ç›®å½•åŠ è½½åˆ†ç±»è§„åˆ™"""
    # å°†è·¯å¾„è½¬æ¢ä¸ºç»å¯¹è·¯å¾„
    abs_path = os.path.join(BASE_DIR, rules_dir)
    category_rules = {}
    if not os.path.isdir(abs_path):
        print(f"ã€è­¦å‘Šã€‘è§„åˆ™ç›®å½• '{abs_path}' ä¸å­˜åœ¨ï¼Œå°†æ— æ³•è¿›è¡Œåˆ†ç±»ï¼")
        return {}
    
    print(f"æ­£åœ¨ä»ã€è§„åˆ™åº“ã€‘'{abs_path}' åŠ è½½åˆ†ç±»è§„åˆ™...")
    for filename in os.listdir(abs_path):
        if filename.endswith('.txt'):
            category_name = os.path.splitext(filename)[0]
            filepath = os.path.join(abs_path, filename)
            keywords = load_list_from_file(filepath)
            if keywords:
                category_rules[category_name] = keywords
                print(f"  - å·²åŠ è½½åˆ†ç±» '{category_name}'ï¼ŒåŒ…å« {len(keywords)} ä¸ªå…³é”®å­—ã€‚")
    return category_rules

# --- å…¨å±€å˜é‡ ---
HEADERS = {}
URL_TEST_TIMEOUT = 8
CATEGORY_RULES = {}
CLOCK_URL = ""

# --- å·¥å…·å‡½æ•°åŒº ---
def load_list_from_file(filename):
    """ä»æ–‡ä»¶åŠ è½½åˆ—è¡¨ï¼Œå»é™¤ç©ºè¡Œå’Œæ³¨é‡Š"""
    # âœ¨ GPSå®šä½ï¼šå°†ç›¸å¯¹è·¯å¾„è½¬æ¢ä¸ºç»å¯¹è·¯å¾„
    abs_path = os.path.join(BASE_DIR, filename)
    if not filename or not os.path.exists(abs_path):
        if filename: print(f"  - é…ç½®æ–‡ä»¶ {abs_path} æœªæ‰¾åˆ°ï¼Œå°†è·³è¿‡ã€‚")
        return []
    try:
        with open(abs_path, 'r', encoding='utf-8') as f:
            return [line.strip() for line in f if line.strip() and not line.startswith('#')]
    except Exception as e:
        print(f"  - è¯»å–é…ç½®æ–‡ä»¶ {abs_path} å¤±è´¥: {e}")
        return []

async def test_url(session, url):
    """æµ‹è¯•å•ä¸ªURLçš„å»¶è¿Ÿï¼Œè¿”å› (url, å»¶è¿Ÿæ¯«ç§’)"""
    try:
        start_time = asyncio.get_event_loop().time()
        async with session.get(url, headers=HEADERS, timeout=URL_TEST_TIMEOUT) as response:
            if 200 <= response.status < 400:
                end_time = asyncio.get_event_loop().time()
                return url, (end_time - start_time) * 1000
            return url, float('inf')
    except (aiohttp.ClientError, asyncio.TimeoutError):
        return url, float('inf')
    except Exception:
        return url, float('inf')

def parse_content(content, ad_keywords):
    """ä»æ–‡æœ¬å†…å®¹ä¸­è§£æå‡ºé¢‘é“åç§°å’ŒURL"""
    channels = {}
    processed_urls = set()

    def add_channel(name, url):
        name = name.strip()
        url = url.strip()
        if not name or not url or url in processed_urls: return
        if any(keyword in name for keyword in ad_keywords): return
        
        if name not in channels: channels[name] = []
        channels[name].append(url)
        processed_urls.add(url)

    lines = content.split('\n')
    for i, line in enumerate(lines):
        line = line.strip()
        if not line or line.startswith('#EXTM3U'): continue
        try:
            if '#genre#' in line: continue
            if line.startswith('#EXTINF:'):
                if i + 1 < len(lines):
                    next_line = lines[i+1].strip()
                    if next_line and not next_line.startswith('#'):
                        url = next_line
                        name_match = re.search(r'tvg-name="([^"]*)"', line)
                        name = name_match.group(1) if name_match else line.split(',')[-1]
                        add_channel(name, url)
            elif ',' in line and 'http' in line:
                last_comma_index = line.rfind(',')
                if last_comma_index != -1:
                    name = line[:last_comma_index]
                    url = line[last_comma_index+1:]
                    if url.startswith('http'): add_channel(name, url)
        except Exception:
            pass
    return channels


async def load_epg_data(epg_url):
    """åŠ è½½å¹¶è§£æEPGæ•°æ®ï¼Œæ”¯æŒgzipå‹ç¼©"""
    if not epg_url: return {}
    print(f"\nåŠ è½½EPGæ•°æ®: {epg_url}...")
    epg_data = {}
    try:
        content_bytes = b''
        async with aiohttp.ClientSession() as session:
            async with session.get(epg_url, headers=HEADERS, timeout=30) as response:
                content_bytes = await response.read()
        
        if content_bytes.startswith(b'\x1f\x8b'):
            content = gzip.decompress(content_bytes).decode('utf-8')
        else:
            content = content_bytes.decode('utf-8')
            
        root = ET.fromstring(content)
        for channel in root.findall('channel'):
            display_name_tag = channel.find('display-name')
            if display_name_tag is not None and display_name_tag.text:
                display_name = display_name_tag.text.strip()
                channel_id = channel.get('id', display_name)
                icon_tag = channel.find('icon')
                logo_url = icon_tag.get('src', "") if icon_tag is not None else ""
                epg_data[display_name] = {"tvg-id": channel_id, "tvg-logo": logo_url}
        print(f"  - EPGåŠ è½½æˆåŠŸï¼å…±è§£æå‡º {len(epg_data)} ä¸ªé¢‘é“çš„èŠ‚ç›®ä¿¡æ¯ã€‚")
    except Exception as e:
        print(f"  - EPGæ•°æ®åŠ è½½å¤±è´¥: {e}")
    return epg_data

def classify_channel(channel_name):
    """æ ¹æ®å…¨å±€è§„åˆ™ä¸ºé¢‘é“ååˆ†ç±»"""
    for category, keywords in CATEGORY_RULES.items():
        if any(keyword in channel_name for keyword in keywords):
            return category
    return "å…¶ä»–"

async def main(args):
    """ä¸»æ‰§è¡Œå‡½æ•°"""
    print(f"æŠ¥å‘Šå“¥å“¥ï¼Œå©‰å„¿çš„â€œè¶…çº§èŠ‚ç›®å•â€ v9.1ã€æœ€ç»ˆæˆå“ã€‘ç‰ˆå¼€å§‹å·¥ä½œå•¦ï¼")
    
    # --- âœ¨âœ¨âœ¨ æ–°å¢ï¼šEPGæºä¼˜é€‰ âœ¨âœ¨âœ¨ ---
    print("\nEPGæºä¼˜é€‰ï¼šæ­£åœ¨æµ‹è¯•æ‰€æœ‰EPGåœ°å€...")
    async with aiohttp.ClientSession() as session:
        epg_tasks = [test_url(session, url) for url in args.epg_url]
        epg_results = await asyncio.gather(*epg_tasks)
    
    valid_epgs = [(url, speed) for url, speed in epg_results if speed != float('inf')]
    if not valid_epgs:
        print("  - è­¦å‘Šï¼šæ‰€æœ‰EPGæºå‡ä¸å¯ç”¨ï¼å°†æ— æ³•åŠ è½½èŠ‚ç›®ä¿¡æ¯ã€‚")
        best_epg_url = ""
        top_3_epgs_str = ""
    else:
        valid_epgs.sort(key=lambda x: x[1])
        top_3_epgs = [url for url, speed in valid_epgs[:3]]
        top_3_epgs_str = ",".join(top_3_epgs)
        best_epg_url = valid_epgs[0][0]
        print(f"  - EPGä¼˜é€‰å®Œæˆï¼æœ¬æ¬¡å°†ä½¿ç”¨: {best_epg_url}")
        print(f"  - æœ€ç»ˆå°†å†™å…¥è¿™å‡ ä¸ªæº: {top_3_epgs_str}")

    ad_keywords = load_list_from_file(args.blacklist)
    favorite_channels = load_list_from_file(args.favorites)
    
    # --- ç¬¬ä¸€æ­¥ï¼šã€ä¸‡æºå½’å®—ã€‘èåˆæ‰€æœ‰æº ---
    print("\nç¬¬ä¸€æ­¥ï¼šã€ä¸‡æºå½’å®—ã€‘æ­£åœ¨èåˆæ‰€æœ‰æº...")
    all_channels_pool = {}
    
    # âœ¨ GPSå®šä½ï¼šè¯»å–æœ¬åœ°æ‰‹åŠ¨æº
    manual_sources_abs_dir = os.path.join(BASE_DIR, args.manual_sources_dir)
    if os.path.isdir(manual_sources_abs_dir):
        print(f"  - è¯»å–ã€ç§å­ä»“åº“ã€‘: {manual_sources_abs_dir}")
        for filename in os.listdir(manual_sources_abs_dir):
            filepath = os.path.join(manual_sources_abs_dir, filename)
            if os.path.isfile(filepath) and filename.endswith(('.txt', '.m3u')):
                with open(filepath, 'r', encoding='utf-8') as f:
                    content = f.read()
                    channels = parse_content(content, ad_keywords)
                    for name, urls in channels.items():
                        if name not in all_channels_pool:
                            all_channels_pool[name] = set()
                        all_channels_pool[name].update(urls)
    
    # âœ¨ GPSå®šä½ï¼šè¯»å–ç½‘ç»œæºåˆ—è¡¨
    remote_sources_abs_file = os.path.join(BASE_DIR, args.remote_sources_file)
    if os.path.exists(remote_sources_abs_file):
        print(f"  - è¯»å–ç½‘ç»œæºæ–‡ä»¶: {remote_sources_abs_file}")
        remote_urls = load_list_from_file(args.remote_sources_file) # load_list_from_fileå†…éƒ¨å·²å¤„ç†ç»å¯¹è·¯å¾„
        async with aiohttp.ClientSession() as session:
            tasks = []
            for url in remote_urls:
                async def fetch_and_parse(remote_url):
                    try:
                        async with session.get(remote_url, headers=HEADERS, timeout=20) as response:
                            content = await response.text(encoding='utf-8', errors='ignore')
                            channels = parse_content(content, ad_keywords)
                            for name, urls in channels.items():
                                if name not in all_channels_pool:
                                    all_channels_pool[name] = set()
                                all_channels_pool[name].update(urls)
                            print(f"    - æˆåŠŸæ‹‰å–å¹¶è§£æ: {remote_url}")
                    except Exception as e:
                        print(f"    - è¯»å–ç½‘ç»œæº {remote_url} å¤±è´¥: {e}")
                tasks.append(fetch_and_parse(url))
            await asyncio.gather(*tasks)

    unique_urls_count = sum(len(urls) for urls in all_channels_pool.values())
    print(f"  - èåˆå®Œæˆï¼å…±æ”¶é›†åˆ° {len(all_channels_pool)} ä¸ªé¢‘é“ï¼Œ{unique_urls_count} ä¸ªä¸é‡å¤åœ°å€ã€‚")

    # --- ç¬¬äºŒæ­¥ï¼šã€ç»ˆæè¯•ç‚¼ã€‘æ£€éªŒæ‰€æœ‰åœ°å€çš„å¯ç”¨æ€§ ---
    print("\nç¬¬äºŒæ­¥ï¼šã€ç»ˆæè¯•ç‚¼ã€‘æ­£åœ¨æ£€éªŒæ‰€æœ‰åœ°å€çš„å¯ç”¨æ€§...")
    all_urls_to_test = {url for urls in all_channels_pool.values() for url in urls}
    
    # âœ¨ GPSå®šä½ï¼šå°†ã€æ¯æ—¥ç²¾é€‰ã€‘çš„æºä¹ŸåŠ å…¥æµ‹è¯•
    picks_abs_dir = os.path.join(BASE_DIR, args.picks_dir)
    if os.path.isdir(picks_abs_dir):
        for pick_file in os.listdir(picks_abs_dir):
            pick_path = os.path.join(picks_abs_dir, pick_file)
            if os.path.isfile(pick_path) and pick_file.endswith(('.txt', '.m3u')):
                with open(pick_path, 'r', encoding='utf-8') as pf:
                    pick_content = pf.read()
                    pick_channels = parse_content(pick_content, ad_keywords)
                    for urls in pick_channels.values():
                        all_urls_to_test.update(urls)

    url_speeds = {}
    async with aiohttp.ClientSession() as session:
        tasks = [test_url(session, url) for url in all_urls_to_test]
        results = await asyncio.gather(*tasks)
        for url, speed in results:
            url_speeds[url] = speed
            
    valid_url_count = sum(1 for speed in url_speeds.values() if speed != float('inf'))
    print(f"  - è¯•ç‚¼å®Œæˆï¼åœ¨ {len(all_urls_to_test)} ä¸ªåœ°å€ä¸­ï¼Œå…±æœ‰ {valid_url_count} ä¸ªå¯ç”¨ã€‚")

    # --- ç¬¬ä¸‰æ­¥ï¼šã€ç”Ÿæ€è¿›åŒ–ã€‘åˆ†ç±»å¹¸å­˜è€…å¹¶ç­›é€‰çº¿è·¯ ---
    print("\nç¬¬ä¸‰æ­¥ï¼šã€ç”Ÿæ€è¿›åŒ–ã€‘æ­£åœ¨ä¸ºå¹¸å­˜è€…åˆ†ç±»å¹¶ç­›é€‰ä¼˜è´¨çº¿è·¯...")
    survivors_classified = {}
    for name, urls in all_channels_pool.items():
        valid_urls = [url for url in urls if url_speeds.get(url, float('inf')) != float('inf')]
        if valid_urls:
            valid_urls.sort(key=lambda u: url_speeds[u])
            category = classify_channel(name)
            if category not in survivors_classified:
                survivors_classified[category] = {}
            survivors_classified[category][name] = valid_urls[:5]

    print(f"  - ç”Ÿæ€è¿›åŒ–å®Œæˆï¼å·²å°†å¹¸å­˜é¢‘é“åˆ†ç±»å¹¶ç­›é€‰å‡ºæœ€ä½³çº¿è·¯ã€‚")

    # --- ç¬¬å››æ­¥ï¼šã€èåˆè¾“å‡ºã€‘æ­£åœ¨ç”Ÿæˆæœ€ç»ˆèŠ‚ç›®å• ---
    print("\nç¬¬å››æ­¥ï¼šã€èåˆè¾“å‡ºã€‘æ­£åœ¨ç”Ÿæˆæœ€ç»ˆèŠ‚ç›®å•...")
    epg_data = await load_epg_data(best_epg_url)
    
    # âœ¨ GPSå®šä½ï¼šç¡®ä¿è¾“å‡ºç›®å½•æ­£ç¡®
    output_abs_path = os.path.join(BASE_DIR, args.output)
    m3u_filename = f"{output_abs_path}.m3u"
    txt_filename = f"{output_abs_path}.txt"
    os.makedirs(os.path.dirname(m3u_filename), exist_ok=True)
    
    beijing_time = datetime.now(timezone(timedelta(hours=8)))
    update_time_str = beijing_time.strftime('%Y-%m-%d %H:%M:%S')


    # 1. å‡†å¤‡ç›²ç›’åˆ†ç»„
    blind_box_group_name = "å©‰å„¿ä¸ºå“¥å“¥æ•´ç†"
    blind_box_channels = {}
    picks_abs_dir = os.path.join(BASE_DIR, args.picks_dir)
    if os.path.isdir(picks_abs_dir):
        print("  - å‘ç°ã€æ¯æ—¥ç²¾é€‰ã€‘ç›²ç›’ï¼Œæ­£åœ¨å‡†å¤‡...")
        pick_files = sorted(os.listdir(picks_abs_dir))
        for pick_file in pick_files:
            pick_path = os.path.join(picks_abs_dir, pick_file)
            if os.path.isfile(pick_path) and pick_file.endswith(('.txt', '.m3u')):
                pick_name = os.path.splitext(pick_file)[0]
                with open(pick_path, 'r', encoding='utf-8') as pf:
                    pick_content = pf.read()
                pick_channels_data = parse_content(pick_content, ad_keywords)
                pick_valid_urls = [url for urls in pick_channels_data.values() for url in urls if url_speeds.get(url, float('inf')) != float('inf')]
                
                if pick_valid_urls:
                    random_url = random.choice(pick_valid_urls)
                    safe_pick_name = pick_name.replace(" ", "-")
                    blind_box_channels[safe_pick_name] = [random_url]
                    print(f"    - ç›²ç›’ '{pick_name}' å·²å¼€å¯ï¼Œå¹¸è¿æºå·²å¤‡å¥½ï¼")
                else:
                    print(f"    - ç›²ç›’ '{pick_name}' ä¸­çš„æ‰€æœ‰æºå‡å·²å¤±æ•ˆã€‚")
    else:
        print("  - æœªæ‰¾åˆ°ã€æ¯æ—¥ç²¾é€‰ã€‘ç›²ç›’ç›®å½• (picks)ï¼Œå°†è·³è¿‡æ­¤åŠŸèƒ½ã€‚")

    # 2. å‡†å¤‡å¸¸è§„åˆ†ç»„
    final_grouped_channels = {}
    if blind_box_channels:
        final_grouped_channels[blind_box_group_name] = blind_box_channels

    for category, channels in survivors_classified.items():
        for name, urls in channels.items():
            group_name = "æˆ‘çš„æœ€çˆ±" if name in favorite_channels else category
            if group_name not in final_grouped_channels:
                final_grouped_channels[group_name] = {}
            if name not in final_grouped_channels[group_name]:
                 final_grouped_channels[group_name][name] = []
            final_grouped_channels[group_name][name].extend(urls)

    # 3. ç¡®å®šæœ€ç»ˆçš„é»„é‡‘æ’åº
    prefix_order = ["å©‰å„¿ä¸ºå“¥å“¥æ•´ç†", "æˆ‘çš„æœ€çˆ±", "å¤®è§†", "å«è§†", "åœ°æ–¹", "æ¸¯æ¾³å°"]
    all_existing_groups = list(final_grouped_channels.keys())
    ordered_groups = []
    
    for group in prefix_order:
        if group in all_existing_groups:
            ordered_groups.append(group)
            all_existing_groups.remove(group)
    
    other_group_exists = "å…¶ä»–" in all_existing_groups
    if other_group_exists:
        all_existing_groups.remove("å…¶ä»–")
    
    ordered_groups.extend(sorted(all_existing_groups))
    
    if other_group_exists:
        ordered_groups.append("å…¶ä»–")

    # 4. æŒ‰ç…§é»„é‡‘é¡ºåºï¼Œç»Ÿä¸€å†™å…¥æ–‡ä»¶
    with open(m3u_filename, 'w', encoding='utf-8') as f_m3u, open(txt_filename, 'w', encoding='utf-8') as f_txt:
        f_m3u.write(f'#EXTM3U x-tvg-url="{top_3_epgs_str}" catchup="append" catchup-source="?playseek=${{(b)yyyyMMddHHmmss}}-${{(e)yyyyMMddHHmmss}}"\n') if top_3_epgs_str else f_m3u.write("#EXTM3U\n")
        f_m3u.write(f'#EXTINF:-1 group-title="æ›´æ–°æ—¶é—´",{update_time_str}\n')
        f_m3u.write(f'{CLOCK_URL}\n')
        
        f_txt.write(f'æ›´æ–°æ—¶é—´,#genre#\n')
        f_txt.write(f'{update_time_str},{CLOCK_URL}\n\n')
        
        for group in ordered_groups:
            channels_in_group = final_grouped_channels.get(group)
            if not channels_in_group: continue
            
            f_txt.write(f'{group},#genre#\n')
            
            for name, urls in sorted(channels_in_group.items()):
                safe_name = name.replace(" ", "-")
                epg_info = epg_data.get(name, epg_data.get(safe_name, {}))
                tvg_id = epg_info.get("tvg-id", safe_name)
                tvg_logo = epg_info.get("tvg-logo", "")
                
                for url in urls:
                    f_txt.write(f'{safe_name},{url}\n')
                    catchup_tag = ""
                    if "PLTV" in url or "TVOD" in url or "/liveplay/" in url or "/replay/" in url:
                        catchup_tag = ' catchup="append" catchup-source="?playseek=${(b)yyyyMMddHHmmss}-${(e)yyyyMMddHHmmss}"'
                    elif ".m3u8" in url and ("playback" in url or "replay" in url):
                         catchup_tag = ' catchup="append" catchup-source="?starttime=${(b)yyyyMMddHHmmss}&endtime=${(e)yyyyMMddHHmmss}"'
                    elif ".php" in url and "id=" in url:
                         catchup_tag = ' catchup="append" catchup-source="&playseek=${(b)yyyyMMddHHmmss}-${(e)yyyyMMddHHmmss}"'

                    f_m3u.write(f'#EXTINF:-1 tvg-id="{tvg_id}" tvg-name="{safe_name}" tvg-logo="{tvg_logo}" group-title="{group}"{catchup_tag},{safe_name}\n')
                    f_m3u.write(f'{url}\n')

            f_txt.write('\n')

    print(f"\nç¬¬äº”æ­¥ï¼šä»»åŠ¡å®Œæˆï¼æˆ‘ä»¬çš„ç”Ÿæ€ç³»ç»Ÿå·²æŒ‰é»„é‡‘é¡ºåºå®Œæˆæœ€ç»ˆè¿›åŒ–ï¼")
    print(f"  - æœ€ç»ˆæˆå“å·²ç”Ÿæˆ: {m3u_filename}")
    print(f"  - TXTç‰ˆæˆå“å·²ç”Ÿæˆ: {txt_filename}")
    print("\nå“¥å“¥ï¼Œå©‰å„¿çš„å·¥ä½œå®Œæˆå•¦ï¼Œå¿«å»äº«å—ä½ çš„ä¸“å±èŠ‚ç›®å•å§ï¼ğŸ¥°")


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='å©‰å„¿çš„â€œè¶…çº§èŠ‚ç›®å•â€ v9.1ã€æœ€ç»ˆæˆå“ã€‘ç‰ˆ')
    
    parser.add_argument('--config', type=str, default='config.json', help='å…¨å±€JSONé…ç½®æ–‡ä»¶çš„è·¯å¾„')
    parser.add_argument('--rules-dir', type=str, default='rules', help='ã€å¤‡ç”¨ã€‘åˆ†ç±»è§„åˆ™ç›®å½•')
    parser.add_argument('--manual-sources-dir', type=str, default='sources_manual', help='ã€ç§å­ä»“åº“ã€‘æ‰‹åŠ¨ç»´æŠ¤çš„æºç›®å½•')
    parser.add_argument('--generated-sources-dir', type=str, default='sources_generated', help='ã€æˆå“ä»“åº“ã€‘è„šæœ¬è‡ªåŠ¨ç”Ÿæˆçš„æºç›®å½•')
    parser.add_argument('--remote-sources-file', type=str, default='sources.txt', help='åŒ…å«è¿œç¨‹ç›´æ’­æºURLåˆ—è¡¨çš„æ–‡ä»¶')
    parser.add_argument('--picks-dir', type=str, default='picks', help='ã€æ¯æ—¥ç²¾é€‰ã€‘ç›²ç›’æºç›®å½•')
    
    parser.add_argument('--epg-url', nargs='+', default=None, help='ã€è¦†ç›–ã€‘EPGæ•°æ®æºURLï¼Œä¼šè¦†ç›–é…ç½®æ–‡ä»¶ä¸­çš„è®¾ç½®')

    parser.add_argument('-b', '--blacklist', type=str, default='config/blacklist.txt', help='é¢‘é“é»‘åå•æ–‡ä»¶')
    parser.add_argument('-f', '--favorites', type=str, default='config/favorites.txt', help='æ”¶è—é¢‘é“åˆ—è¡¨æ–‡ä»¶')
    parser.add_argument('-o', '--output', type=str, default='dist/live', help='è¾“å‡ºæ–‡ä»¶çš„å‰ç¼€ï¼ˆä¸å«æ‰©å±•åï¼‰')
    
    args = parser.parse_args()

    config = load_global_config(args.config)
    
    if 'category_rules' in config and isinstance(config['category_rules'], dict):
        print("æ­£åœ¨ä» config.json åŠ è½½åˆ†ç±»è§„åˆ™...")
        CATEGORY_RULES = config['category_rules']
    else:
        print("config.json ä¸­æœªæ‰¾åˆ°åˆ†ç±»è§„åˆ™ï¼Œå°†ä» 'rules' ç›®å½•åŠ è½½ã€‚")
        CATEGORY_RULES = load_category_rules_from_dir(args.rules_dir)

    epg_source_list = []
    if args.epg_url:
         epg_source_list = args.epg_url
         print("æ£€æµ‹åˆ°å‘½ä»¤è¡ŒEPGå‚æ•°ï¼Œä¼˜å…ˆä½¿ç”¨ï¼")
    elif 'epg_urls' in config and isinstance(config['epg_urls'], list):
         epg_source_list = config['epg_urls']
         print("æ­£åœ¨ä» config.json åŠ è½½EPGæºåˆ—è¡¨...")
    else:
         epg_source_list = ['https://live.fanmingming.com/e.xml']
         print("æœªæ‰¾åˆ°ä»»ä½•EPGé…ç½®ï¼Œä½¿ç”¨å†…ç½®å¤‡ç”¨åœ°å€ã€‚")
    args.epg_url = epg_source_list

    HEADERS = config.get('headers', {})
    URL_TEST_TIMEOUT = config.get('url_test_timeout', 8)
    CLOCK_URL = config.get('clock_url', "")
    
    try:
        asyncio.run(main(args))
    except KeyboardInterrupt:
        print("\næ”¶åˆ°å“¥å“¥çš„æŒ‡ä»¤ï¼Œç¨‹åºæå‰ç»“æŸã€‚")
